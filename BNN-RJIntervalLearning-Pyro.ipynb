{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch \n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from torch import nn\n",
    "from pyro import poutine\n",
    "from pyro.nn.module import PyroModule, PyroParam, PyroSample, to_pyro_module_\n",
    "from pyro.infer.enum import config_enumerate\n",
    "from pyro.infer.mcmc import NUTS, MCMC, HMC\n",
    "from pyro.infer.mcmc.mcmc_kernel import MCMCKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "x_line = np.linspace(0, 20, 500)\n",
    "y_line = x_line * 5 + 20 + (2 * np.random.randn(500))\n",
    "x1 = np.random.normal(2, 1, 100)\n",
    "x2 = np.random.normal(6, 1, 100)\n",
    "x3 = np.random.normal(11, 1, 100)\n",
    "#x = np.concatenate((x1, x2))\n",
    "#x = x1 + x2\n",
    "\n",
    "plt.scatter(x_line, y_line)\n",
    "#sns.distplot(x)\n",
    "x = torch.tensor(x_line).float()\n",
    "y = torch.tensor(y_line).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-Gaussian Data\n",
    "#Demonstrate a model that learns the mixing proportion (delta) of two Gaussians\n",
    "delta = 0.8\n",
    "N = 1000\n",
    "x1 = np.random.normal(7, 0.5, int((delta*N)))\n",
    "x2 = np.random.normal(10, 0.5, int((1-delta)*N))\n",
    "x = np.concatenate((x1, x2))\n",
    "x = torch.tensor(x).float()\n",
    "sns.distplot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "#A helper function for viewing model/sampling outputs\n",
    "def get_params(trace):\n",
    "    parameter_samples = {}\n",
    "    for name, node in trace.iter_stochastic_nodes():\n",
    "        if (node['type'] == 'sample' and node['is_observed'] == False):\n",
    "            parameter_samples[name] = node[\"value\"].detach()\n",
    "    return parameter_samples\n",
    "\n",
    "def model(x):\n",
    "    delta = pyro.sample('delta',dist.Beta(2,2))\n",
    "    changepoint = delta * len(x)\n",
    "    mask = torch.arange(len(x), dtype=torch.long) >= changepoint\n",
    "    dist1 = dist.Normal(7, 0.5)\n",
    "    dist2 = dist.Normal(10, 0.5)\n",
    "    with pyro.plate('data', len(x)):\n",
    "        pyro.sample('obs', dist.MaskedMixture(mask, dist1, dist2), obs=x)\n",
    "    \n",
    "guide = poutine.block(model, hide_types=[\"is_observed\"])\n",
    "\n",
    "old_state = None\n",
    "\n",
    "logs = []\n",
    "params = []\n",
    "params2 = []\n",
    "params3 = []\n",
    "\n",
    "#Basic MH \n",
    "for i in range(10000):\n",
    "    \n",
    "    conditioned_model = poutine.condition(model, data={\"obs\": x})\n",
    "    new_state = poutine.trace(guide).get_trace(x)\n",
    "    if i == 0:\n",
    "        old_state = new_state\n",
    "    \n",
    "    model_tr_prop = poutine.trace(poutine.replay(conditioned_model, trace=new_state)).get_trace(x)\n",
    "    model_tr_current = poutine.trace(poutine.replay(conditioned_model, trace=old_state)).get_trace(x)\n",
    "    #probs = model_tr_prop\n",
    "    lp1 = np.exp(model_tr_prop.log_prob_sum().detach().numpy())\n",
    "    lp2 = np.exp(model_tr_current.log_prob_sum().detach().numpy())\n",
    "\n",
    "    #log_ratio = (lp1/lp2)\n",
    "    log_ratio = np.exp(model_tr_prop.log_prob_sum().detach().numpy() - model_tr_current.log_prob_sum().detach().numpy())\n",
    "    logs.append(log_ratio)\n",
    "    if log_ratio >= 1:\n",
    "        old_state = new_state\n",
    "        #print(\"yes\")\n",
    "    else:\n",
    "        rand_un = torch.rand(1)\n",
    "        if log_ratio > rand_un:\n",
    "            old_state = new_state\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    state = get_params(old_state)\n",
    "    params.append(state['delta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(params[100:])\n",
    "np.mean(params[100:])\n",
    "#plt.plot(logs[400:])\n",
    "#model_tr_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "def model(x, y=None):\n",
    "    mu1 = pyro.sample('mu1',dist.Normal(0,10))\n",
    "    beta1 = pyro.sample('beta1',dist.Normal(0,10))\n",
    "    fitted = pyro.deterministic('fitted',value=(mu1 * x + beta1))\n",
    "    with pyro.plate('data', len(x)):\n",
    "        pyro.sample('obs', dist.Normal(fitted, 2), obs=y)\n",
    "\n",
    "guide = poutine.block(model, hide_types=[\"is_observed\"])\n",
    "\n",
    "def guide(x, y=None):\n",
    "    loc = torch.tensor(0).float()\n",
    "    scale = torch.tensor(10).float()\n",
    "    mu1l = pyro.param('mu1l',loc)\n",
    "    mu1s = pyro.param('mu1s',scale)\n",
    "    beta1l = pyro.param('beta1l',loc)\n",
    "    beta1s = pyro.param('beta1s',scale)\n",
    "    mu1 = pyro.sample('mu1', dist.Normal(mu1l, mu1s))\n",
    "    beta1 = pyro.sample('beta1',dist.Normal(beta1l,beta1s))\n",
    "    fitted = pyro.deterministic('fitted',value=(mu1 * x + beta1))\n",
    "\n",
    "def get_params(trace):\n",
    "    parameter_samples = {}\n",
    "    for name, node in trace.iter_stochastic_nodes():\n",
    "        if (node['type'] == 'sample' and node['is_observed'] == False):\n",
    "            parameter_samples[name] = node[\"value\"].detach()\n",
    "    return parameter_samples\n",
    "\n",
    "old_state = None\n",
    "model(x, y)\n",
    "\n",
    "logs = []\n",
    "params = []\n",
    "params2 = []\n",
    "params3 = []\n",
    "for i in range(40000):\n",
    "    \n",
    "    conditioned_model = poutine.condition(model, data={\"obs\": y})\n",
    "    guide_tr = poutine.trace(guide).get_trace(x)\n",
    "    if i == 0:\n",
    "        old_state = guide_tr\n",
    "    state = get_params(old_state)\n",
    "    model_tr_prop = poutine.trace(poutine.replay(conditioned_model, trace=guide_tr)).get_trace(x,y)\n",
    "    model_tr_current = poutine.trace(poutine.replay(conditioned_model, trace=old_state)).get_trace(x,y)\n",
    "    #probs = model_tr_prop\n",
    "    elbo1 = np.exp(model_tr_prop.log_prob_sum().detach().numpy()/500)\n",
    "    elbo2 = np.exp(model_tr_current.log_prob_sum().detach().numpy()/500)\n",
    "    #print(elbo1/elbo2)\n",
    "    if elbo1/elbo2 >= 1:\n",
    "        old_state = guide_tr\n",
    "        logs.append(elbo1.item())\n",
    "    else:\n",
    "        rand_un = torch.rand(1)\n",
    "        if elbo1/elbo2 > rand_un:\n",
    "            old_state = guide_tr\n",
    "            logs.append(elbo1.item())\n",
    "        else:\n",
    "            logs.append(elbo2.item())\n",
    "\n",
    "    \n",
    "    params.append(state['mu1'])\n",
    "    params2.append(state['beta1'])\n",
    "\n",
    "#plt.scatter(params,logs)\n",
    "#plt.plot(params)\n",
    "#print(logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(params)\n",
    "plt.plot(params2)\n",
    "#plt.plot(params3)\n",
    "\n",
    "print(np.mean(params))\n",
    "print(np.mean(params2[1000:]))\n",
    "#print(np.mean(params3))\n",
    "test = pyro.get_param_store()['mu1l']\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "x = torch.tensor([0., 1., 10., 11., 12.])\n",
    "#sns.distplot(data)\n",
    "\n",
    "K = 2  # Fixed number of components.\n",
    "\n",
    "#@config_enumerate\n",
    "def model(data):\n",
    "    # Global variables.\n",
    "    weights = torch.tensor([0.5,0.5]) #pyro.sample('weights', dist.Dirichlet(0.5 * torch.ones(K)))\n",
    "    scale = pyro.sample('scale', dist.LogNormal(0., 2.))\n",
    "    with pyro.plate('components', K):\n",
    "        locs = pyro.sample('locs', dist.Normal(0., 10.))\n",
    "\n",
    "    with pyro.plate('data', len(data)):\n",
    "        # Local variables.\n",
    "        assignment = pyro.sample('assignment', dist.Categorical(weights))\n",
    "        pyro.sample('obs', dist.Normal(locs[assignment], scale), obs=data)\n",
    "\n",
    "guide = poutine.block(model, hide_types=[\"is_observed\"])\n",
    "def guide(data):\n",
    "    # Global variables.\n",
    "    weights = torch.tensor([0.5,0.5]) #pyro.sample('weights', dist.Dirichlet(0.5 * torch.ones(K)))\n",
    "    scale = pyro.sample('scale', dist.LogNormal(0., 2.))\n",
    "    with pyro.plate('components', K):\n",
    "        locs = pyro.sample('locs', dist.Normal(0., 10.))\n",
    "\n",
    "    with pyro.plate('data', len(data)):\n",
    "        # Local variables.\n",
    "        assignment = pyro.sample('assignment', dist.Categorical(weights))\n",
    "\n",
    "logs = []\n",
    "params = []\n",
    "params2 = []\n",
    "params3 = []\n",
    "\n",
    "for i in range(10000):\n",
    "    \n",
    "    conditioned_model = poutine.condition(model, data={\"obs\": x})\n",
    "    new_state = poutine.trace(guide).get_trace(x)\n",
    "    if i == 0:\n",
    "        old_state = new_state\n",
    "    \n",
    "    model_tr_prop = poutine.trace(poutine.replay(conditioned_model, trace=new_state)).get_trace(x)\n",
    "    model_tr_current = poutine.trace(poutine.replay(conditioned_model, trace=old_state)).get_trace(x)\n",
    "    #probs = model_tr_prop\n",
    "    lp1 = np.exp(model_tr_prop.log_prob_sum().detach().numpy())\n",
    "    lp2 = np.exp(model_tr_current.log_prob_sum().detach().numpy())\n",
    "\n",
    "    #log_ratio = (lp1/lp2)\n",
    "    log_ratio = np.exp(model_tr_prop.log_prob_sum().detach().numpy() - model_tr_current.log_prob_sum().detach().numpy())\n",
    "    logs.append(log_ratio)\n",
    "    if log_ratio >= 1:\n",
    "        old_state = new_state\n",
    "        print(\"yes\")\n",
    "    else:\n",
    "        rand_un = torch.rand(1)\n",
    "        if log_ratio > rand_un:\n",
    "            old_state = new_state\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    state = get_params(old_state)\n",
    "    params.append(state['components'])\n",
    "    params2.append(state['assignment'])\n",
    "    params3.append(state['locs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locs1 = []\n",
    "locs2 = []\n",
    "for i in params3:\n",
    "    locs1.append(i[0])\n",
    "    locs2.append(i[1])\n",
    "\n",
    "plt.plot(locs1)\n",
    "plt.plot(locs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################\n",
    "#Reversible Jumps\n",
    "##################\n",
    "\n",
    "#Learn an Interval - Actual RJ Time\n",
    "y0 = np.repeat(0, 3)\n",
    "y1 = np.repeat(0.25, 7)\n",
    "y2 = np.repeat(0.5, 7)\n",
    "y3 = np.repeat(0.75, 3)\n",
    "y4 = np.repeat(1.0, 5)\n",
    "\n",
    "y = np.concatenate((y0, y1, y2, y3, y4))\n",
    "x = [10 * ((y - 0.5) + 2)]\n",
    "y = torch.tensor(y).float()\n",
    "x = torch.t(torch.tensor(x).float())\n",
    "print(y.size())\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def get_params(trace):\n",
    "    parameter_samples = {}\n",
    "    for name, node in trace.iter_stochastic_nodes():\n",
    "        if (node['type'] == 'sample' and node['is_observed'] == False):\n",
    "            parameter_samples[name] = node[\"value\"].detach()\n",
    "    return parameter_samples\n",
    "    \n",
    "def split_k(k):\n",
    "    intervals = [round(i*(float(1/k)),3) for i in range(k+1)]\n",
    "    return intervals\n",
    "\n",
    "test_k = split_k(4)\n",
    "print(test_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Make a very simple Bayes Net\n",
    "class BNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=2):\n",
    "        super(BNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = nn.functional.sigmoid(out)\n",
    "        out = self.fc2(out)\n",
    "        out = nn.functional.sigmoid(out)\n",
    "        return(out)\n",
    "\n",
    "net = BNN()\n",
    "to_pyro_module_(net)\n",
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def model(x, y):\n",
    "    fc1w_prior = dist.Normal(loc=torch.zeros_like(net.fc1.weight), scale=torch.ones_like(net.fc1.weight)*3)\n",
    "    fc1b_prior = dist.Normal(loc=torch.zeros_like(net.fc1.bias), scale=torch.ones_like(net.fc1.bias)*3)\n",
    "    fc2w_prior = dist.Normal(loc=torch.zeros_like(net.fc2.weight), scale=torch.ones_like(net.fc2.weight)*3)\n",
    "    fc2b_prior = dist.Normal(loc=torch.zeros_like(net.fc2.bias), scale=torch.ones_like(net.fc2.bias)*3)\n",
    "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, 'fc2.weight': fc2w_prior, 'fc2.bias': fc2b_prior}\n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    lhat = lifted_reg_model(x)\n",
    "\n",
    "    k = 4\n",
    "    interval = split_k(k)\n",
    "    assignments = torch.tensor([min(interval, key=lambda x:abs(x-y)) for y in lhat]).float()\n",
    "\n",
    "    with pyro.plate('data', len(y)):\n",
    "        # Local variables.\n",
    "        obs = pyro.sample('obs', dist.Normal(assignments, 0.1), obs=y)\n",
    "    \n",
    "    return obs\n",
    "\n",
    "def guide(x, y):\n",
    "    #First layer weight distribution priors\n",
    "    fc1w_mu = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_sigma = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n",
    "    fc1w_sigma_param = softplus(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n",
    "    fc1w_prior = dist.Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)\n",
    "    # First layer bias distribution priors\n",
    "    fc1b_mu = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_sigma = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
    "    fc1b_sigma_param = softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
    "    fc1b_prior = dist.Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
    "    #Second layer weight distribution priors\n",
    "    fc2w_mu = torch.randn_like(net.fc2.weight)\n",
    "    fc2w_sigma = torch.randn_like(net.fc2.weight)\n",
    "    fc2w_mu_param = pyro.param(\"fc2w_mu\", fc2w_mu)\n",
    "    fc2w_sigma_param = softplus(pyro.param(\"fc2w_sigma\", fc2w_sigma))\n",
    "    fc2w_prior = dist.Normal(loc=fc2w_mu_param, scale=fc2w_sigma_param)\n",
    "    #Second layer bias distribution priors\n",
    "    fc2b_mu = torch.randn_like(net.fc2.bias)\n",
    "    fc2b_sigma = torch.randn_like(net.fc2.bias)\n",
    "    fc2b_mu_param = pyro.param(\"fc2b_mu\", fc2b_mu)\n",
    "    fc2b_sigma_param = softplus(pyro.param(\"fc2b_sigma\", fc2b_sigma))\n",
    "    fc2b_prior = dist.Normal(loc=fc2b_mu_param, scale=fc2b_sigma_param)\n",
    "\n",
    "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, 'fc2.weight': fc2w_prior, 'fc2.bias': fc2b_prior}\n",
    "    \n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    lhat = lifted_reg_model(x)\n",
    "\n",
    "    return lhat\n",
    "\n",
    "z = model(x,y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel = HMC(model)\n",
    "mcmc = MCMC(kernel, num_samples=500)\n",
    "mcmc.run(x, y)\n",
    "samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Sampling Loop\n",
    "logs = []\n",
    "params = []\n",
    "params2 = []\n",
    "params3 = []\n",
    "\n",
    "for i in range(300):\n",
    "    \n",
    "    conditioned_model = poutine.condition(model, (x, y))\n",
    "    new_state = poutine.trace(guide).get_trace(x, y)\n",
    "    if i == 0:\n",
    "        old_state = new_state\n",
    "    \n",
    "    model_tr_prop = poutine.trace(poutine.replay(conditioned_model, trace=new_state)).get_trace(x, y)\n",
    "    model_tr_current = poutine.trace(poutine.replay(conditioned_model, trace=old_state)).get_trace(x, y)\n",
    "    #probs = model_tr_prop\n",
    "    lp1 = np.exp(model_tr_prop.log_prob_sum().detach().numpy())\n",
    "    lp2 = np.exp(model_tr_current.log_prob_sum().detach().numpy())\n",
    "\n",
    "    #log_ratio = (lp1/lp2)\n",
    "    log_ratio = np.exp(model_tr_prop.log_prob_sum().detach().numpy() - model_tr_current.log_prob_sum().detach().numpy())\n",
    "    if log_ratio > 1:\n",
    "        log_ratio = 1\n",
    "    logs.append(log_ratio)\n",
    "    if log_ratio >= 1:\n",
    "        old_state = new_state\n",
    "        params.append(get_params(old_state)['module$$$fc1.weight'])\n",
    "        params2.append(get_params(old_state)['module$$$fc1.bias'])\n",
    "    else:\n",
    "        rand_un = torch.rand(1)\n",
    "        if log_ratio > rand_un:\n",
    "            old_state = new_state\n",
    "            params.append(get_params(old_state)['module$$$fc1.weight'])\n",
    "            params2.append(get_params(old_state)['module$$$fc1.bias'])\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    #params.append(get_params(old_state))\n",
    "    #params.append(get_params(old_state)['module$$$fc1.weight'])\n",
    "    #params2.append(get_params(old_state)['module$$$fc1.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
