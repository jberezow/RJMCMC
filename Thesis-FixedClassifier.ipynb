{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "using Gen\n",
    "using PyPlot\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Flux\n",
    "using Random\n",
    "using Distances\n",
    "include(\"hmc_mod.jl\")\n",
    "include(\"helper_functions.jl\")\n",
    "include(\"rj_proposals_nodes.jl\")\n",
    "include(\"NUTS.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------\n",
    "#Hyperparameters and Helper Functions\n",
    "#------------------------------------\n",
    "\n",
    "#Select Network Goal\n",
    "network = \"classifier\"\n",
    "#network = \"interpolator\"\n",
    "\n",
    "#Data hyperparameters\n",
    "n = 20 #Number of samples per mode (classifier)\n",
    "m = 4 #Number of modes (classifier)\n",
    "d = 2 #Input dimension\n",
    "N = n*m #Total samples\n",
    "σₐ = 0.03 #Mode variance (classifier)\n",
    "bound = 0.5\n",
    "\n",
    "#Network hyperparameters\n",
    "\n",
    "#Node hyperparameters\n",
    "k_range = 4 #Maximum number of neurons per layer\n",
    "k_list = [Int(i) for i in 1:k_range]\n",
    "k_real = 2\n",
    "\n",
    "#NUTS\n",
    "Δmax = 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "x_raw, classes = real_data_classifier(Int(N/4), 4, bound, σₐ);\n",
    "classes = [(i+1) % 2 + 1 for i in classes]\n",
    "y_real = classes\n",
    "\n",
    "plot_data_classifier(x_raw,classes)\n",
    "x = transpose(x_raw)\n",
    "size(x)\n",
    "typeof(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Neural Net\n",
    "function G(x, trace)\n",
    "    activation = σ\n",
    "    layers = 1 #trace[:l]\n",
    "    ks = [trace[(:k,i)] for i=1:layers]\n",
    "    for i=1:layers\n",
    "        in_dim, out_dim = layer_unpacker(i, layers, ks)\n",
    "        W = reshape(trace[(:W,i)], out_dim, in_dim)\n",
    "        b = reshape(trace[(:b,i)], trace[(:k,i)])\n",
    "        nn = Dense(W, b, activation)\n",
    "        x = nn(x)\n",
    "    end\n",
    "    \n",
    "    Wₒ = reshape(trace[(:W,layers+1)], 1, ks[layers])\n",
    "    bₒ = reshape(trace[(:b,layers+1)], 1)\n",
    "    \n",
    "    nn_out = Dense(Wₒ, bₒ)\n",
    "    return nn_out(x)\n",
    "end;\n",
    "\n",
    "@gen function classifier(x)\n",
    "    \n",
    "    #Create a blank choicemap\n",
    "    obs = choicemap()::ChoiceMap\n",
    "    \n",
    "    #Draw number of layers\n",
    "    l = 1\n",
    "    \n",
    "    #Create individual weight and bias vectors\n",
    "    #Loop through hidden layers\n",
    "    k = [Int(0) for i=1:l+1]\n",
    "    for i=1:l\n",
    "        k[i] = @trace(categorical([1/length(k_list) for i=1:length(k_list)]), (:k,i))\n",
    "        obs[(:k,i)] = k[i]\n",
    "    end\n",
    "    k[l+1] = @trace(categorical([1.0]), (:k,l+1))\n",
    "    obs[(:k,l+1)] = k[l+1]\n",
    "    \n",
    "    ######################################\n",
    "    #New hyperparameter schedule - Jan 20#\n",
    "    ######################################\n",
    "    \n",
    "    #Standard Deviations\n",
    "    τ₁ ~ gamma(100,0.001) #Hidden weights and biases\n",
    "    τ₂ ~ gamma(100*k[1],0.001) #Output weights and biases\n",
    "    σ₁ = 1/τ₁\n",
    "    σ₂ = 1/τ₂\n",
    "    \n",
    "    #Sample weight and parameter vectors\n",
    "    W = [zeros(k[i]) for i=1:l+1]\n",
    "    b = [zeros(k[i]) for i=1:l+1]\n",
    "    μ = [zeros(k[i]) for i=1:l+1]\n",
    "    μb = [zeros(k[i]) for i=1:l+1]\n",
    "   \n",
    "     for i=1:l+1\n",
    "        if i == 1\n",
    "            h = Int(d * k[i])\n",
    "        else\n",
    "            h = Int(k[i-1] * k[i])\n",
    "        end\n",
    "\n",
    "        if i<=l\n",
    "            #Hidden Weights\n",
    "            u = zeros(h) #Draw\n",
    "            S = Diagonal([1 for i=1:length(u)])\n",
    "            μ[i] = @trace(mvnormal(u,S), (:μ,i))\n",
    "            Σ = Diagonal([σ₁ for i=1:length(μ[i])])\n",
    "            W[i] = @trace(mvnormal(μ[i],Σ), (:W,i))\n",
    "            obs[(:W,i)] = W[i]\n",
    "            \n",
    "            #Hidden Biases\n",
    "            ub = zeros(k[i]) #Draw\n",
    "            Sb = Diagonal([1 for i=1:length(ub)])    \n",
    "            μb[i] = @trace(mvnormal(ub,Sb), (:μb,i))\n",
    "            Σ2 = Diagonal([σ₁ for i=1:length(μb[i])])\n",
    "            b[i] = @trace(mvnormal(μb[i],Σ2), (:b,i))\n",
    "            obs[(:b,i)] = b[i]\n",
    "        else\n",
    "            #Output Weights\n",
    "            u = zeros(k[l]) #Draw\n",
    "            S = Diagonal([1 for i=1:length(u)])\n",
    "            μ[i] = @trace(mvnormal(u,S), (:μ,i))\n",
    "            Σ = Diagonal([σ₂ for i=1:length(μ[i])])\n",
    "            W[i] = @trace(mvnormal(μ[i],Σ), (:W,i))\n",
    "            obs[(:W,i)] = W[i]\n",
    "\n",
    "            #Output Bias\n",
    "            ub = zeros(1) #Draw\n",
    "            Sb = Diagonal([1 for i=1:length(ub)])  \n",
    "            μb[i] = @trace(mvnormal(ub,Sb), (:μb,i))\n",
    "            Σ2 = Diagonal([σ₂ for i=1:length(μb[i])])\n",
    "            b[i] = @trace(mvnormal(μb[i],Σ2), (:b,i))\n",
    "            obs[(:b,i)] = b[i]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #Return Network Scores for X\n",
    "    scores = G(x,obs)\n",
    "    scores = Flux.σ.(scores)\n",
    "    \n",
    "    #Logistic Regression Likelihood\n",
    "    y = scores\n",
    "    for j=1:N\n",
    "        y[j] = @trace(categorical([1-scores[j],scores[j]]), (:y,j))\n",
    "    end\n",
    "    #y = [(@trace(categorical([1-scores[j],scores[j]]), (:y,j))) for j=1:length(scores)]\n",
    "\n",
    "    return scores\n",
    "end;\n",
    "obs_master = choicemap()::ChoiceMap\n",
    "for i=1:length(classes)\n",
    "    obs_master[(:y,i)] = classes[i]\n",
    "end\n",
    "obs = obs_master;\n",
    "(best_trace,) = generate(classifier, (x,), obs)\n",
    "\n",
    "println(best_trace[:τ₁])\n",
    "println(best_trace[:τ₂])\n",
    "\n",
    "test_scores = classifier(x)\n",
    "test_labels = data_labeller(test_scores)\n",
    "test_acc = sum([classes[i] == test_labels[i] for i=1:length(classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------\n",
    "#Test Likelihood\n",
    "#----------------\n",
    "scores = []\n",
    "accs = []\n",
    "ks = []\n",
    "best_ks = []\n",
    "best_traces = []\n",
    "(best_trace,) = generate(classifier, (x,), obs)\n",
    "best_acc = 0\n",
    "best_score = get_score(best_trace)\n",
    "best_pred_y = (G(x, best_trace))\n",
    "best_pred_labels = data_labeller(best_pred_y)\n",
    "best_k = best_trace[(:k,1)]\n",
    "function likelihood(best_trace, best_acc, best_score, best_k)\n",
    "    obs = obs_master;\n",
    "    #obs[(:k,1)] = 2\n",
    "    (trace,) = generate(classifier, (x,), obs)\n",
    "    \n",
    "    pred_y = (G(x, trace))\n",
    "    pred_labels = data_labeller(pred_y)\n",
    "    acc = sum([classes[i] == pred_labels[i] for i=1:length(classes)])\n",
    "    score = get_score(trace)\n",
    "\n",
    "    if acc > best_acc\n",
    "        best_acc = acc\n",
    "        best_score = score\n",
    "        best_trace = trace\n",
    "        best_pred_y = pred_y\n",
    "        best_k = best_trace[(:k,1)]\n",
    "    end\n",
    "    push!(best_ks,best_k)\n",
    "    push!(scores,score)\n",
    "    push!(accs,acc)\n",
    "\n",
    "    return(best_trace, best_acc, best_score, best_k)\n",
    "end;\n",
    "\n",
    "for i=1:10000\n",
    "    best_trace, best_acc, best_score, best_k = likelihood(best_trace, best_acc, best_score, best_k)\n",
    "    push!(best_ks, best_k)\n",
    "end\n",
    "\n",
    "PyPlot.scatter(accs, scores)\n",
    "plt.title(\"Comparing Classifier Accuracy to Log Likelihood\")\n",
    "plt.xlabel(\"Classifier Accuracy\")\n",
    "plt.ylabel(\"Log Likelihood\")\n",
    "plt.ylim(-100,1)\n",
    "plt.legend()\n",
    "#print(best_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Helper Functions\n",
    "include(\"hmc_mod.jl\")\n",
    "\n",
    "scores = []\n",
    "traces = []\n",
    "ks = []\n",
    "\n",
    "function propose_hyperparameters(trace)\n",
    "    hyper_selection = select()\n",
    "    push!(hyper_selection, :τ₁)\n",
    "    push!(hyper_selection, :τ₂)\n",
    "    (new_trace, weight, retdiff) = regenerate(trace, hyper_selection)\n",
    "    if log(rand()) < weight\n",
    "        return new_trace\n",
    "    else\n",
    "        return trace\n",
    "    end\n",
    "end;\n",
    "\n",
    "function propose_parameters(trace)\n",
    "    param_selection = select()\n",
    "    for i=1:1+1 #Number of Layers\n",
    "        push!(param_selection, (:μ,i))\n",
    "        push!(param_selection, (:μb,i))\n",
    "        push!(param_selection, (:W,i))\n",
    "        push!(param_selection, (:b,i))\n",
    "    end\n",
    "    (new_trace, weight) = regenerate(trace, param_selection)\n",
    "    if log(rand()) < weight\n",
    "        return new_trace\n",
    "    else\n",
    "        return trace\n",
    "    end\n",
    "end;\n",
    "\n",
    "function hmc_parameters(trace)\n",
    "    \n",
    "    param_selection = select()\n",
    "    for i=1:1+1 #Number of Layers\n",
    "        push!(param_selection, (:μ,i))\n",
    "        push!(param_selection, (:μb,i))\n",
    "        push!(param_selection, (:W,i))\n",
    "        push!(param_selection, (:b,i))\n",
    "    end\n",
    "    \n",
    "    L = 10\n",
    "    eps = 0.1\n",
    "    \n",
    "    (trace, accepted) = hmc(trace,param_selection,L=L,eps=eps,check=false,observations=obs)\n",
    "    return(trace)\n",
    "end\n",
    "\n",
    "function node_parameter(trace)\n",
    "    obs = obs_master\n",
    "    \n",
    "    node_selection = select()\n",
    "    for i=1:1+1 #Number of Layers\n",
    "        push!(node_selection, (:k,i))\n",
    "        push!(node_selection, (:μ,i))\n",
    "        push!(node_selection, (:μb,i))\n",
    "        push!(node_selection, (:W,i))\n",
    "        push!(node_selection, (:b,i))\n",
    "    end\n",
    "    \n",
    "    param_selection = select()\n",
    "    for i=1:1+1 #Number of Layers\n",
    "        push!(param_selection, (:μ,i))\n",
    "        push!(param_selection, (:μb,i))\n",
    "        push!(param_selection, (:W,i))\n",
    "        push!(param_selection, (:b,i))\n",
    "    end\n",
    "    \n",
    "    L = 100\n",
    "    eps = 0.2\n",
    "    \n",
    "    (trace_tilde, hmc_score) = hmc_mod(trace,param_selection,L=L,eps=eps,check=false,observations=obs)\n",
    "    (trace_prime, q_weight) = regenerate(trace_tilde, node_selection)\n",
    "    (trace_star, hmc_score2) = hmc_mod(trace_prime,param_selection,L=L,eps=eps,check=false,observations=obs)\n",
    "    \n",
    "    println(q_weight)\n",
    "    \n",
    "    theta_score = get_score(trace)\n",
    "    star_score = get_score(trace_star)\n",
    "    across_score = star_score - theta_score + hmc_score + hmc_score2 #+ q_weight\n",
    "\n",
    "    if rand() < exp(across_score)\n",
    "        return trace_star\n",
    "    else\n",
    "        return trace\n",
    "    end\n",
    "end\n",
    "\n",
    "(trace,) = generate(classifier, (x,), obs)\n",
    "trace2 = node_parameter(trace)\n",
    "\n",
    "for i=1:1000\n",
    "    trace = node_parameter(trace)\n",
    "    trace = propose_hyperparameters(trace)\n",
    "    push!(scores,get_score(trace))\n",
    "    push!(traces, trace)\n",
    "    push!(ks, trace[(:k,1)])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.69459862670641e-23\n",
      "7.69459862670642e-23\n"
     ]
    }
   ],
   "source": [
    "a = Distributions.logpdf(Normal(0,1), 10.0)\n",
    "b = pdf(Normal(0,1), 10.0)\n",
    "\n",
    "println(exp(a))\n",
    "println(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------\n",
    "#K Plot\n",
    "#-------\n",
    "plot(ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trace,) = generate(classifier, (x,), obs)\n",
    "println(get_score(trace))\n",
    "trace = propose_hyperparameters(trace)\n",
    "println(get_score(trace))\n",
    "scores = []\n",
    "traces = []\n",
    "\n",
    "for i=1:1000\n",
    "    trace = node_parameter(trace)\n",
    "    trace = propose_hyperparameters(trace)\n",
    "    push!(scores,get_score(trace))\n",
    "    push!(traces, trace)\n",
    "end\n",
    "\n",
    "plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------\n",
    "#Score plot\n",
    "#-----------\n",
    "\n",
    "scores = [get_score(trace) for trace in traces]\n",
    "plot(scores)\n",
    "plt.ylim(-200,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------\n",
    "#Accuracy plot\n",
    "#---------------\n",
    "\n",
    "accs = []\n",
    "for i=1:length(traces)\n",
    "    trace = traces[i]\n",
    "    pred_y = G(x,trace)\n",
    "    pred_labels = data_labeller(pred_y)\n",
    "    \n",
    "    acc = sum([classes[i] == pred_labels[i] for i=1:length(classes)])\n",
    "    push!(accs,acc)\n",
    "end\n",
    "\n",
    "plot(accs)\n",
    "println(sum(accs)/length(accs))\n",
    "plt.title(\"RJMCMC Accuracy: XOR Classifier\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"# Classified Correctly (out of 200)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------\n",
    "#Classifier plot\n",
    "#---------------\n",
    "\n",
    "function plot_grid(data,scores,alpha=1.0)\n",
    "    PyPlot.scatter(data[:,1],data[:,2],c=scores,alpha=alpha,cmap=\"PRGn\")\n",
    "    #PyPlot.colorbar()\n",
    "end\n",
    "\n",
    "function tracegrid(traces, samples=500, low=-1.0, high=1.0)\n",
    "    d=2\n",
    "    n=100\n",
    "    r = range(low, high, length = n)\n",
    "    \n",
    "    iter = Iterators.product((r for _ in 1:d)...)\n",
    "    grid= vec([collect(i) for i in iter])\n",
    "    grid_raw = reduce(hcat, getindex.(grid,i) for i in eachindex(grid[1]))\n",
    "    grid2 = transpose(grid_raw)\n",
    "    z_master = zeros(length(grid2[1,:]))\n",
    "    \n",
    "    for i=1:samples\n",
    "        j = rand((100,length(traces)))\n",
    "        trace = traces[j]\n",
    "        z = Flux.σ.(G(grid2,trace))[1,:]\n",
    "        z_master += (z ./ samples)\n",
    "    end\n",
    "    plot_grid(grid_raw, z_master)\n",
    "end\n",
    "\n",
    "tracegrid(traces)\n",
    "plot_data_classifier(x_raw,classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
