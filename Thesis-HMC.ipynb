{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library Calls\n",
    "using Gen\n",
    "using LinearAlgebra\n",
    "using Distributions\n",
    "using Random\n",
    "using PyPlot\n",
    "using Flux;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'll make my own damn HMC sampler, with blackjack and hookers\n",
    "#Page 14 of the Neal HMC Chapter (HB of MCMC)\n",
    "function jon_hmc(trace, ϵ, L, observations=EmptyChoiceMap())\n",
    "    current_q = trace[:Θ]\n",
    "    U = get_score(trace)\n",
    "    q = current_q\n",
    "    p = rand(Normal(0,1),length(q))\n",
    "    current_p = p\n",
    "    \n",
    "    #Get Gradients\n",
    "    (values, gradient) = grad_U(trace)\n",
    "    \n",
    "    #Make a half-step for momentum at the beginning\n",
    "    p = p - ϵ * gradient / 2\n",
    "    \n",
    "    #Alternate full steps for position and momentum\n",
    "    for i=1:L\n",
    "       #Make a full step for the position\n",
    "        q = q + ϵ * p\n",
    "        #Make a full step for the momentum, except at end of trajectory\n",
    "        p = i!=L ? p - ϵ * grad_U(q) : p\n",
    "    end\n",
    "    \n",
    "    #Make a half-step for momentum at the end\n",
    "    p = p - ϵ * grad_U(q) / 2\n",
    "    \n",
    "    #Negate momentum at end of trajectory to make the proposal symmetric\n",
    "    p = -p\n",
    "    \n",
    "    #Evaluate potential and kinetic energies at start and end of trajectory\n",
    "    current_U = U(current_q)\n",
    "    current_K = sum(current_p^2)\n",
    "    proposed_U = U(q)\n",
    "    proposed_K = sum(p^2) / 2\n",
    "    \n",
    "    #Accept or reject the state at the end of the trajectory. Return either\n",
    "    #the position at the end, or the initial position\n",
    "    if rand(Uniform(0,1),1) < exp(current_U - proposed_U + current_K - proposed_K) #EQ 3.16 PG 12\n",
    "        return(q) #ACCEPT\n",
    "    else\n",
    "        return(current_q) #REJECT\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There's very little meat in these auxiliary momentum variables\n",
    "#U(q) = -log[π(q)L(q|D)] - Negative \n",
    "function U(trace)\n",
    "    return(get_score(trace)/length(get_retval(trace)))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See all those gradients, Homer? Thats why your HMC sampler didn't work!\n",
    "function grad_U(trace)\n",
    "    (_, values_trie, gradient_trie) = choice_gradients(trace, select(:Θ), nothing)\n",
    "    values = to_array(values_trie, Float64)\n",
    "    gradient = to_array(gradient_trie, Float64)\n",
    "    return (values, gradient)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at those hyperparameters fly\n",
    "n = 500\n",
    "N = 1200\n",
    "σₐ = 0.002\n",
    "k_range = 12\n",
    "k_list = [i for i in 1:k_range];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I am helping to make your boots go faster\n",
    "function plot_data(data,classes)\n",
    "    markers = [\"o\",\"*\"]\n",
    "    colors = [\"blue\",\"green\"]\n",
    "    for i=1:2\n",
    "        mask = [classes[j] == i for j in 1:length(classes)]\n",
    "        scatter(data[:,1][mask],data[:,2][mask],c=colors[i],marker=markers[i],zorder=3)\n",
    "    end\n",
    "end;\n",
    "\n",
    "function unpack(Θ,k)\n",
    "    m = length(Θ)\n",
    "    h = 2*k\n",
    "    Wₕ = reshape(Θ[1:h], k, 2);   \n",
    "    bₕ = reshape(Θ[h+1:h+k], k)\n",
    "    bₕ = bₕ\n",
    "    \n",
    "    Wₒ = reshape(Θ[h+k+1:m-1], 1, k);\n",
    "    bₒ = Θ[m]\n",
    "      \n",
    "    return Wₕ, bₕ, Wₒ, bₒ\n",
    "end\n",
    "\n",
    "function data_labeller(y::Array{Float64})\n",
    "    labels = [y[i] > 0.5 ? 2 : 1 for i=1:length(y)]\n",
    "    return labels\n",
    "end\n",
    "\n",
    "function sigmoid(x::Array{Float64}, a=10000)\n",
    "    return 1.0 ./ (1.0 .+ a.*exp.(-x))\n",
    "end\n",
    "\n",
    "function hardσ(x, a=2)\n",
    "    max(0, min(1.0, a * x + 0.5))\n",
    "end\n",
    "    \n",
    "#function sigmoid(x::Float64, a=1)\n",
    "    #return 1.0 / 1.0 + a*exp(-x)\n",
    "#end\n",
    "\n",
    "mₖ(k) = k*4 + 1\n",
    "\n",
    "#Test out the unpack method\n",
    "k = 5\n",
    "m = mₖ(k)\n",
    "θtest = [i for i=1:m]\n",
    "a,b,c,d = unpack(θtest,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulated Data\n",
    "function real_data(N::Int, modes::Int, σ::Float64)\n",
    "    μ₁ = [0.25, 0.25]\n",
    "    μ₂ = [0.25, 0.75]\n",
    "    μ₃ = [0.75, 0.75]\n",
    "    μ₄ = [0.75, 0.25]\n",
    "    #μ₅ = [1.25, 1.25]\n",
    "    #μ₆ = [1.25, 1.75]\n",
    "    #μ₇ = [1.75, 1.75]\n",
    "    #μ₈ = [1.75, 1.25]\n",
    "    μ = [μ₁, μ₂, μ₃, μ₄]\n",
    "    Σ = [[σ, 0] [0, σ]]\n",
    "    \n",
    "    all_samples = zeros(Float64, (N*modes, 2))\n",
    "    classes = zeros(Int, (N*modes))\n",
    "    \n",
    "    for i = 1:modes\n",
    "        dist = MvNormal(μ[i], Σ)\n",
    "        sample = rand(dist, N)::Matrix\n",
    "        #scatter(sample[1,:],sample[2,:])\n",
    "        all_samples[(i-1)*N+1:i*N,:] = transpose(sample)\n",
    "        classes[(i-1)*N+1:i*N] = fill(i, N)\n",
    "        classes = float(classes)\n",
    "    end\n",
    "    return all_samples, classes\n",
    "end\n",
    "    \n",
    "data_raw, classes = real_data(Int(N/4), 4, σₐ);\n",
    "classes = [(i+1) % 2 + 1 for i in classes]\n",
    "\n",
    "plot_data(data_raw,classes)\n",
    "data = transpose(data_raw)\n",
    "size(data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Neural Net\n",
    "function G(x, Θ::AbstractVector, k::Int)\n",
    "    if(typeof(x) == Float64)\n",
    "        x = [x]\n",
    "    end\n",
    "    W₁, b₁, W₂, b₂ = unpack(Θ,k)\n",
    "    nn = Chain(Dense(W₁, b₁, tanh),\n",
    "               Dense(W₂, [b₂], hardσ))\n",
    "    ps = Flux.params(nn)\n",
    "    return nn(x), ps\n",
    "end;\n",
    "\n",
    "@dist k_count(r,p,min) = neg_binom(r,p) + 1\n",
    "\n",
    "@gen function classifier(x::Array{Float64})\n",
    "    σₖ = 100.0::Float64\n",
    "    \n",
    "    # Create the weight and bias vector.\n",
    "    #k ~ uniform_discrete(1,k_range)\n",
    "    #k ~ k_count(1,0.5,1)\n",
    "    k = 2\n",
    "    m = mₖ(k)\n",
    "    μ = zeros(m)\n",
    "    Σ = Diagonal([σₖ for i=1:m])\n",
    "    Θ ~ mvnormal(μ,Σ) #Coefficients of the polynomial\n",
    "    \n",
    "    #Return Network Scores for X\n",
    "    #W₁, b₁, W₂, b₂ = unpack(Θ,k)\n",
    "    #h = 2*k\n",
    "    \n",
    "    #nn = Chain(Dense(reshape(Θ[1:h], k, 2), reshape(Θ[h+1:h+k], k), tanh),\n",
    "               #Dense(reshape(Θ[h+k+1:m-1], 1, k), [Θ[m]], hardσ))\n",
    "    \n",
    "    #ps = Flux.params(nn)\n",
    "    #scores = nn(x)\n",
    "    scores, _ = G(x,Θ,k)\n",
    "    #scores = sigmoid(nn_out,10000)\n",
    "    \n",
    "    #Logistic Regression Likelihood\n",
    "    y = zeros(length(x[1,:])) #Array to fill with class labels\n",
    "    for j=1:N\n",
    "        y[j] = @trace(categorical([1-scores[j],scores[j]]), (:y,j))\n",
    "    end\n",
    "    return y\n",
    "end;\n",
    "\n",
    "#z = classifier(data)\n",
    "\n",
    "#k = 4\n",
    "#m = mₖ(k)\n",
    "#θ = ones(m)\n",
    "#out, ps = G(data, θ, k)\n",
    "#gs = Flux.gradient(() -> sum(G(data,θ,k)), ps)\n",
    "#Flux.gradient(G, out, ps, k)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Gen.DynamicDSLTrace{DynamicDSLFunction{Any}}(DynamicDSLFunction{Any}(Dict{Symbol,Any}(), Dict{Symbol,Any}(), Type[Array{Float64,N} where N], false, Union{Nothing, Some{Any}}[nothing], ##classifier#372, Bool[0], false), Trie{Any,Gen.ChoiceOrCallRecord}(Dict{Any,Gen.ChoiceOrCallRecord}((:y, 909) => Gen.ChoiceOrCallRecord{Int64}(2, 0.0, NaN, true),(:y, 875) => Gen.ChoiceOrCallRecord{Int64}(1, -Inf, NaN, true),(:y, 1048) => Gen.ChoiceOrCallRecord{Int64}(2, 0.0, NaN, true),(:y, 969) => Gen.ChoiceOrCallRecord{Int64}(2, 0.0, NaN, true),(:y, 684) => Gen.ChoiceOrCallRecord{Int64}(1, -Inf, NaN, true),(:y, 224) => Gen.ChoiceOrCallRecord{Int64}(1, -Inf, NaN, true),(:y, 389) => Gen.ChoiceOrCallRecord{Int64}(2, 0.0, NaN, true),(:y, 718) => Gen.ChoiceOrCallRecord{Int64}(1, -Inf, NaN, true),(:y, 606) => Gen.ChoiceOrCallRecord{Int64}(1, -Inf, NaN, true),(:y, 557) => Gen.ChoiceOrCallRecord{Int64}(2, 0.0, NaN, true)…), Dict{Any,Trie{Any,Gen.ChoiceOrCallRecord}}()), false, -Inf, 0.0, ([0.25537297720085655 0.20620262286560137 … 0.6619458854826169 0.7663441777564126; 0.14605726841532957 0.24040711277906068 … 0.19667030261043186 0.24043483000806276],), [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]), -Inf)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Register Observed Data\n",
    "obs = choicemap()::ChoiceMap\n",
    "for j in 1:length(classes)\n",
    "    obs[(:y, j)] = classes[j]\n",
    "end;\n",
    "\n",
    "(trace,) = generate(classifier, (data,), obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((nothing,), DynamicChoiceMap(Dict{Any,Any}(:Θ => [2.7371906069815632, 10.727774464033388, -9.692723885804783, 5.005455437821508, 5.6968575063845055, 9.74819510585455, -8.015955308751465, 19.288561719714195, -2.0170398019193274]), Dict{Any,Any}()), DynamicChoiceMap(Dict{Any,Any}(:Θ => [NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN]), Dict{Any,Any}()))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(_, values_trie, gradient_trie) = choice_gradients(trace, select(:Θ), nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function line_model(xs::Vector{Float64})\n",
    "    n = length(xs)\n",
    "    slope = @trace(normal(0,1), :slope)\n",
    "    intercept = @trace(normal(0,2), :intercept)\n",
    "    \n",
    "    for i = 1:length(xs)\n",
    "        #println(slope * xs[i] + intercept)\n",
    "        @trace(normal(slope*xs[i] + intercept, 0.1),(:y,i))\n",
    "    end\n",
    "    \n",
    "    return n\n",
    "end;\n",
    "xs = [-5.,-4.,-3.,-2.,-1.,0.,1.,2.,3.,4.,5.]\n",
    "\n",
    "n = line_model(xs)\n",
    "println(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = simulate(line_model, (xs,))\n",
    "get_gen_fn(trace).params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
