{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gen\n",
    "using Distributions\n",
    "using PyPlot\n",
    "using Random\n",
    "using Flux\n",
    "using LinearAlgebra\n",
    "include(\"helper_functions.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------\n",
    "#Hyperparameters and Helper Functions\n",
    "#------------------------------------\n",
    "\n",
    "#Select Network Goal\n",
    "network = \"classifier\"\n",
    "#network = \"interpolator\"\n",
    "\n",
    "#Data hyperparameters\n",
    "n = 20 #Number of samples per mode (classifier)\n",
    "m = 4 #Number of modes (classifier)\n",
    "d = 2 #Input dimension\n",
    "N = n*m #Total samples\n",
    "σₐ = 0.02 #Mode variance (classifier)\n",
    "\n",
    "#Network hyperparameters\n",
    "α = 6 #Gamma Scale for Hyperparameters\n",
    "\n",
    "#Node hyperparameters\n",
    "k_range = 12 #Maximum number of neurons per layer\n",
    "k_list = [2]\n",
    "k_real = 2\n",
    "\n",
    "#NUTS Max Gradient\n",
    "Δmax = 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "x_raw, classes = real_data_classifier(Int(N/4), 4, σₐ);\n",
    "classes = [(i+1) % 2 + 1 for i in classes]\n",
    "y_real = classes\n",
    "\n",
    "plot_data_classifier(x_raw,classes)\n",
    "x = transpose(x_raw)\n",
    "size(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Neural Net\n",
    "function G(x, trace)\n",
    "    activation = relu\n",
    "    layers = 1\n",
    "    ks = [trace[(:k,i)] for i=1:layers]\n",
    "    for i=1:layers\n",
    "        in_dim, out_dim = layer_unpacker(i, layers, ks)\n",
    "        W = reshape(trace[(:W,i)], out_dim, in_dim)\n",
    "        b = reshape(trace[(:b,i)], trace[(:k,i)])\n",
    "        nn = Dense(W, b, activation)\n",
    "        x = nn(x)\n",
    "    end\n",
    "    \n",
    "    Wₒ = reshape(trace[(:W,layers+1)], 1, ks[layers])\n",
    "    bₒ = reshape(trace[(:b,layers+1)], 1)\n",
    "    \n",
    "    nn_out = Dense(Wₒ, bₒ)\n",
    "    return nn_out(x)\n",
    "end;\n",
    "\n",
    "@gen function classifier(x::Array{Float64})\n",
    "    \n",
    "    #Create a blank choicemap\n",
    "    obs = choicemap()::ChoiceMap\n",
    "    \n",
    "    #Draw number of layers\n",
    "    l = 1\n",
    "    \n",
    "    #Create individual weight and bias vectors\n",
    "    #Loop through hidden layers\n",
    "    k = [Int(0) for i=1:l+1]\n",
    "    k[1] = @trace(categorical([0.0,1.0]), (:k,1))\n",
    "    obs[(:k,1)] = k[1]\n",
    "    #for i=1:l\n",
    "        #k[i] = @trace(categorical([1/length(k_list) for i=1:length(k_list)]), (:k,i))\n",
    "        #obs[(:k,i)] = k[i]\n",
    "    #end\n",
    "    k[l+1] = @trace(categorical([1.0]), (:k,l+1))\n",
    "    obs[(:k,l+1)] = k[l+1]\n",
    "    \n",
    "    α = 0.001 #Gamma Scale for Hyperparameters\n",
    "    \n",
    "    ω₁ = 100\n",
    "    ω₂ = (sum([obs[(:k,i)] for i=1:l]))*100 #Neal (1996): Scaling relationship to # of hidden units\n",
    "    τ₁ ~ gamma(ω₁,α) #Hidden Weights\n",
    "    τ₂ ~ gamma(ω₁,α) #Hidden Biases\n",
    "    τ₃ ~ gamma(ω₂,α) #Output Weights\n",
    "    \n",
    "    #Standard Deviations\n",
    "    σ₁ = 1/τ₁\n",
    "    σ₂ = 1/τ₂\n",
    "    σ₃ = 1/τ₃\n",
    "    #σᵧ = sqrt(1/τᵧ)\n",
    "    \n",
    "    #Sample weight and parameter vectors\n",
    "    W = [zeros(k[i]) for i=1:l+1]\n",
    "    b = [zeros(k[i]) for i=1:l+1]\n",
    "    for i=1:l+1\n",
    "        if i == 1\n",
    "            h = Int(d * k[i])\n",
    "        else\n",
    "            h = Int(k[i-1] * k[i])\n",
    "        end\n",
    "\n",
    "        if i<=l\n",
    "            #Hidden Weights\n",
    "            μ = zeros(h)\n",
    "            Σ = Diagonal([σ₁ for i=1:length(μ)])\n",
    "            W[i] = @trace(mvnormal(μ,Σ), (:W,i))\n",
    "            obs[(:W,i)] = W[i]\n",
    "            \n",
    "            #Hidden Biases\n",
    "            μ2 = ones(k[i])\n",
    "            Σ2 = Diagonal([σ₂ for i=1:length(μ2)])\n",
    "            b[i] = @trace(mvnormal(μ2,Σ2), (:b,i))\n",
    "            obs[(:b,i)] = b[i]\n",
    "        else\n",
    "            #Output Weights\n",
    "            μₒ = zeros(k[l])\n",
    "            Σₒ = Diagonal([σ₃ for i=1:length(μₒ)])\n",
    "            W[i] = @trace(mvnormal(μₒ,Σₒ), (:W,i))\n",
    "            obs[(:W,i)] = W[i]\n",
    "\n",
    "            #Output Bias\n",
    "            μ2ₒ = ones(1)\n",
    "            Σ2ₒ = Diagonal([1.0 for i=1:length(μ2ₒ)])\n",
    "            b[i] = @trace(mvnormal(μ2ₒ,Σ2ₒ), (:b,i))\n",
    "            obs[(:b,i)] = b[i]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #Return Network Scores for X\n",
    "    scores = G(x,obs)\n",
    "    scores = Flux.σ.(scores)\n",
    "    \n",
    "    #Logistic Regression Likelihood\n",
    "    y = zeros(length(scores))\n",
    "    for j=1:N\n",
    "        y[j] = @trace(categorical([1-scores[j],scores[j]]), (:y,j))\n",
    "    end\n",
    "\n",
    "    return scores\n",
    "    \n",
    "end;\n",
    "\n",
    "#(best_trace,) = generate(classifier, (x,), obs)\n",
    "#println(best_trace[:τ₁])\n",
    "#println(best_trace[:τ₂])\n",
    "#println(best_trace[:τ₃])\n",
    "\n",
    "test_scores = classifier(x)\n",
    "test_labels = data_labeller(test_scores)\n",
    "test_acc = sum([classes[i] == test_labels[i] for i=1:length(classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "function select_hyperparameters(trace, obs)\n",
    "    args = get_args(trace)\n",
    "    argdiffs = map((_) -> NoChange(), args)\n",
    "    (new_trace,weight,retdiff) = regenerate(trace, args, argdiffs, select(:τ₁,:τ₂,:τ₃))\n",
    "    obs[:τ₁] = new_trace[:τ₁]\n",
    "    obs[:τ₂] = new_trace[:τ₂]\n",
    "    obs[:τ₃] = new_trace[:τ₃]\n",
    "    return new_trace, obs\n",
    "end\n",
    "\n",
    "function select_selection_NUTS(trace)\n",
    "    l = 1\n",
    "    selection = select()\n",
    "    for i=1:l+1\n",
    "        push!(selection, (:W,i))\n",
    "        push!(selection, (:b,i))\n",
    "    end\n",
    "    return selection\n",
    "end\n",
    "\n",
    "function run_NUTS(trace, iters, obs)\n",
    "    for i=1:iters\n",
    "        \n",
    "\n",
    "        #(new_trace, hmc_score) = NUTS(trace, selection, check=false, observations=obs, M=1)\n",
    "        #if rand(Uniform(0,1)) < exp(hmc_score)\n",
    "            #trace = new_trace\n",
    "            #accepted = true\n",
    "        #else\n",
    "            #accepted = false\n",
    "        #end\n",
    "        #push!(traces, trace)\n",
    "        #accepted && println(\"Within accepted\")\n",
    "    end\n",
    "    return trace\n",
    "end\n",
    "\n",
    "obs_master = choicemap()::ChoiceMap\n",
    "for i=1:length(classes)\n",
    "    obs_master[(:y,i)] = classes[i]\n",
    "end\n",
    "obs = obs_master;\n",
    "(trace,) = generate(classifier, (x,), obs)\n",
    "\n",
    "#run_NUTS(trace, 1, obs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sample_momenta(n::Int)\n",
    "    Float64[random(normal, 0, 1) for _=1:n]\n",
    "end\n",
    "\n",
    "function assess_momenta(momenta)\n",
    "    logprob = 0.\n",
    "    for val in momenta\n",
    "        logprob += Gen.logpdf(normal, val, 0, 1)\n",
    "    end\n",
    "    logprob\n",
    "end\n",
    "\n",
    "function NUTS(trace, ϵ, check=false, observations = obs, M=1::Int)\n",
    "    #Get θ₀\n",
    "    Θ = []\n",
    "    args = get_args(trace)\n",
    "    retval_grad = accepts_output_grad(get_gen_fn(trace)) ? zero(get_retval(trace)) : nothing\n",
    "    argdiffs = map((_) -> NoChange(), args)\n",
    "    selection = select_selection_NUTS(trace)\n",
    "    (_, values_trie, gradient_trie) = choice_gradients(trace, selection, retval_grad)\n",
    "    θ₀ = to_array(values_trie, Float64)\n",
    "    push!(Θ, θ₀)\n",
    "    C_choice = 0\n",
    "    \n",
    "    prev_model_score = get_score(trace)\n",
    "    r₀ = sample_momenta(length(θ₀))\n",
    "    prev_momenta_score = assess_momenta(r₀)\n",
    "        \n",
    "    #Loop M times\n",
    "    for m=1:M\n",
    "        #Resample Position Variables\n",
    "        r = r₀\n",
    "        θ = from_array(values_trie, θ₀)\n",
    "        (new_trace, _, _) = update(trace, args, argdiffs, θ)\n",
    "        score = exp(get_score(new_trace) - 0.5(dot(r,r)))\n",
    "        u = rand(Uniform(0,score))\n",
    "        \n",
    "        #Initialize\n",
    "        θ⁻ = Θ[m]\n",
    "        θ⁺ = Θ[m]\n",
    "        r⁻ = r₀\n",
    "        r⁺ = r₀\n",
    "        j = 0\n",
    "        C = Set(tuple([Θ[m], r₀]))\n",
    "        s = 1\n",
    "        \n",
    "        while s == 1\n",
    "            vⱼ = rand([-1,1])\n",
    "            if vⱼ == -1\n",
    "                θ⁻,r⁻,_,_,C¹,s¹ = build_tree(trace,selection,θ⁻,r⁻,u,vⱼ,j,ϵ)\n",
    "            else\n",
    "                _,_,θ⁺,r⁺,C¹,s¹ = build_tree(trace,selection,θ⁺,r⁺,u,vⱼ,j,ϵ)\n",
    "            end\n",
    "            if s¹ == 1\n",
    "                C = union(C,C¹)\n",
    "            end\n",
    "            i¹ = (dot((θ⁺ - θ⁻),r⁻) ≥ 0) ? 1 : 0\n",
    "            i² = (dot((θ⁺ - θ⁻),r⁺) ≥ 0) ? 1 : 0\n",
    "            s = s¹*i¹*i²\n",
    "            j += 1\n",
    "        end\n",
    "        C_choice = rand(unique(C))\n",
    "    end\n",
    "    θ = from_array(values_trie, C_choice[1])\n",
    "    print\n",
    "    momenta = C_choice[2]\n",
    "    (new_trace, _, _) = update(trace, args, argdiffs, θ)\n",
    "    \n",
    "    # assess new model score (negative potential energy)\n",
    "    new_model_score = get_score(new_trace)\n",
    "\n",
    "    # assess new momenta score (negative kinetic energy)\n",
    "    new_momenta_score = assess_momenta(-momenta)\n",
    "\n",
    "    # accept or reject\n",
    "    alpha = new_model_score - prev_model_score + new_momenta_score - prev_momenta_score\n",
    "    (new_trace, alpha)\n",
    "end\n",
    "\n",
    "obs_master = choicemap()::ChoiceMap\n",
    "for i=1:length(classes)\n",
    "    obs_master[(:y,i)] = classes[i]\n",
    "end\n",
    "obs = obs_master;\n",
    "(trace,) = generate(classifier, (x,), obs);\n",
    "trace2, test = NUTS(trace, 0.01, false, obs, 1)\n",
    "trace3, test = NUTS(trace2, 0.01, false, obs, 1)\n",
    "println(get_score(trace))\n",
    "println(get_score(trace2))\n",
    "println(get_score(trace3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = Set(tuple([ones(9), zeros(9)]))\n",
    "D = Set(tuple([\"Test2\", 2]))\n",
    "F = Set(tuple([\"Test\", 1]))\n",
    "E = union(C,D)\n",
    "\n",
    "println(sizeof(C))\n",
    "\n",
    "rand(unique(E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function build_tree(trace,selection,θ,r,u,v,j,ϵ)\n",
    "    #Base case - take one leapfrog step in the direction of v\n",
    "    if j == 0\n",
    "        θ¹,r¹ = leapfrog(trace,selection,θ,r,v*ϵ)\n",
    "        (_, values_trie, _) = choice_gradients(trace, selection, retval_grad)\n",
    "        θm = from_array(values_trie, θ¹)\n",
    "        (tree_trace, _, _) = update(trace, args, argdiffs, θm)\n",
    "        score = get_score(tree_trace) - 0.5(dot(r¹,r¹))\n",
    "        if u ≤ exp(score)\n",
    "            C¹ = Set(tuple([θ¹,r¹]))\n",
    "        else\n",
    "            C¹ = Set()\n",
    "        end\n",
    "        s¹ = (score > log(u) - Δmax) ? 1 : 0\n",
    "        return θ¹,r¹,θ¹,r¹,C¹,s¹\n",
    "    #Recursion - build left and right subtrees\n",
    "    else\n",
    "        θ⁻,r⁻,θ⁺,r⁺,C¹,s¹ = build_tree(trace,selection,θ,r,u,v,j-1,ϵ)\n",
    "        if v == -1\n",
    "            θ⁻,r⁻,_,_,C²,s² = build_tree(trace,selection,θ⁻,r⁻,u,v,j-1,ϵ)\n",
    "        else\n",
    "            _,_,θ⁺,r⁺,C²,s² = build_tree(trace,selection,θ⁺,r⁺,u,v,j-1,ϵ)\n",
    "        end\n",
    "        i¹ = (dot((θ⁺ - θ⁻),r⁻) ≥ 0) ? 1 : 0\n",
    "        i² = (dot((θ⁺ - θ⁻),r⁺) ≥ 0) ? 1 : 0\n",
    "        s¹ = s¹*s²*i¹*i²\n",
    "        C¹ = union(C¹,C²)\n",
    "       return θ⁻,r⁻,θ⁺,r⁺,C¹,s¹\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function leapfrog(trace,selection,θ,r,ϵ)\n",
    "    new_trace = trace\n",
    "    (_, values_trie, _) = choice_gradients(new_trace, selection, retval_grad)\n",
    "    θtrace = from_array(values_trie, θ)\n",
    "    (new_trace, _, _) = update(new_trace, args, argdiffs, θtrace)\n",
    "    (_, values_trie, gradient_trie) = choice_gradients(new_trace, selection, retval_grad)\n",
    "    gradient = to_array(gradient_trie, Float64)\n",
    "    for step=1:1\n",
    "        # half step on momenta\n",
    "        r += (ϵ / 2) * gradient\n",
    "\n",
    "        # full step on positions\n",
    "        θ += ϵ .* r\n",
    "\n",
    "        # get new gradient\n",
    "        values_trie = from_array(values_trie, θ)\n",
    "        (new_trace, _, _) = update(new_trace, args, argdiffs, values_trie)\n",
    "        (_, _, gradient_trie) = choice_gradients(new_trace, selection, retval_grad)\n",
    "        gradient = to_array(gradient_trie, Float64)\n",
    "\n",
    "        # half step on momenta\n",
    "        r += (ϵ / 2) * gradient\n",
    "    end\n",
    "    return θ,r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_master = choicemap()::ChoiceMap\n",
    "for i=1:length(classes)\n",
    "    obs_master[(:y,i)] = classes[i]\n",
    "end\n",
    "obs = obs_master;\n",
    "(trace,) = generate(classifier, (x,), obs);\n",
    "args = get_args(trace)\n",
    "retval_grad = accepts_output_grad(get_gen_fn(trace)) ? zero(get_retval(trace)) : nothing\n",
    "argdiffs = map((_) -> NoChange(), args)\n",
    "selection = select_selection_NUTS(trace)\n",
    "leapfrog(trace,selection,ones(9),ones(9),0.12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
