{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "using Gen\n",
    "using PyPlot\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Flux\n",
    "using Random\n",
    "using Distances\n",
    "using JLD\n",
    "using StatsBase\n",
    "include(\"hmc_mod.jl\")\n",
    "include(\"helper_functions.jl\")\n",
    "include(\"rj_proposals.jl\")\n",
    "include(\"NUTS.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------\n",
    "#Load Boston Housing Dataset\n",
    "#---------------------------\n",
    "data = load(\"boston.jld\")[\"boston\"]\n",
    "\n",
    "# Generating test/training sets:\n",
    "nrow, ncol = size(data)\n",
    "nrow_test  = div(nrow, 3)\n",
    "nrow_train = nrow - nrow_test\n",
    "\n",
    "x = data[:,1:13]\n",
    "y = data[:,14]\n",
    "\n",
    "dx = fit(UnitRangeTransform, x; dims=1, unit=true)\n",
    "StatsBase.transform!(dx, x)\n",
    "dy = fit(UnitRangeTransform, y; dims=1, unit=true)\n",
    "StatsBase.transform!(dy, y);\n",
    "\n",
    "x_raw = x\n",
    "x = transpose(x)\n",
    "\n",
    "#------------------------------------\n",
    "#Hyperparameters and Helper Functions\n",
    "#------------------------------------\n",
    "\n",
    "#Select Network Goal\n",
    "#network = \"classifier\"\n",
    "network = \"interpolator\"\n",
    "\n",
    "#Data hyperparameters\n",
    "n = nrow #Number of samples per mode (classifier)\n",
    "d = ncol-1 #Input dimension\n",
    "\n",
    "#Network hyperparameters\n",
    "k_real = 8 #Number of hidden nodes per layer\n",
    "k_vector = [0.0 for i=1:k_real]\n",
    "k_vector[k_real] = 1.0\n",
    "\n",
    "#Layer hyperparameters\n",
    "l_range = 4 #Maximum number of layers in the network\n",
    "l_list = [Int(i) for i in 1:l_range]\n",
    "l_real = 1\n",
    "\n",
    "#NUTS\n",
    "Δmax = 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Neural Net\n",
    "function G(x, trace)\n",
    "    activation = relu\n",
    "    layers = trace[:l]\n",
    "    ks = [trace[(:k,i)] for i=1:layers]\n",
    "    for i=1:layers\n",
    "        in_dim, out_dim = layer_unpacker(i, layers, ks)\n",
    "        W = reshape(trace[(:W,i)], out_dim, in_dim)\n",
    "        b = reshape(trace[(:b,i)], trace[(:k,i)])\n",
    "        nn = Dense(W, b, activation)\n",
    "        x = nn(x)\n",
    "    end\n",
    "    \n",
    "    Wₒ = reshape(trace[(:W,layers+1)], 1, ks[layers])\n",
    "    bₒ = reshape(trace[(:b,layers+1)], 1)\n",
    "    \n",
    "    nn_out = Dense(Wₒ, bₒ)\n",
    "    return nn_out(x)\n",
    "end;\n",
    "\n",
    "@gen function interpolator(x::Array{Float64})\n",
    "    \n",
    "    #Create a blank choicemap\n",
    "    obs = choicemap()::ChoiceMap\n",
    "    \n",
    "    #Draw number of layers\n",
    "    l ~ categorical([1/length(l_list) for i=1:length(l_list)])\n",
    "    l_real = l\n",
    "    obs[:l] = l\n",
    "    \n",
    "    #Create individual weight and bias vectors\n",
    "    #Loop through hidden layers\n",
    "    k = [Int(0) for i=1:l+1]\n",
    "    for i=1:l\n",
    "        k[i] = @trace(categorical(k_vector), (:k,i))\n",
    "        obs[(:k,i)] = k[i]\n",
    "    end\n",
    "    k[l+1] = @trace(categorical([1.0]), (:k,l+1))\n",
    "    obs[(:k,l+1)] = k[l+1]\n",
    "    \n",
    "    α₁ = 0.001 #Gamma Scale for Hyperparameters\n",
    "    α₂ = 0.1\n",
    "    α₃ = 1.0\n",
    "    αᵧ = 0.01\n",
    "    \n",
    "    ω₁ = 100\n",
    "    ω₂ = k_real*100 #Neal (1996): Scaling relationship to # of hidden units\n",
    "    ωᵧ = k_real*100\n",
    "    τ₁ ~ gamma(ω₁,α₁) #Hidden Weights\n",
    "    τ₂ ~ gamma(ω₁,α₂) #Hidden Biases\n",
    "    τ₃ ~ gamma(ω₂,α₃) #Output Weights\n",
    "    τᵧ ~ gamma(ωᵧ,α₃) #Noise Parameter for y\n",
    "    #τ₄ ~ gamma() #Output Biases - Neal uses fixed sigmas here\n",
    "    \n",
    "    #Standard Deviations\n",
    "    σ₁ = 1/τ₁\n",
    "    σ₂ = 1/τ₂\n",
    "    σ₃ = 1/τ₃\n",
    "    σᵧ = 1/τᵧ\n",
    "    \n",
    "    #println(σᵧ)\n",
    "    \n",
    "    #Sample weight and parameter vectors\n",
    "    W = [zeros(k[i]) for i=1:l+1]\n",
    "    b = [zeros(k[i]) for i=1:l+1]\n",
    "    for i=1:l+1\n",
    "        if i == 1\n",
    "            h = Int(d * k[i])\n",
    "        else\n",
    "            h = Int(k[i-1] * k[i])\n",
    "        end\n",
    "\n",
    "        if i<=l\n",
    "            #Hidden Weights\n",
    "            μ = zeros(h)\n",
    "            if i == 1\n",
    "                Σ = Diagonal([σ₁ for i=1:length(μ)])\n",
    "            else\n",
    "                Σ = Diagonal([σ₃ for i=1:length(μ)])\n",
    "            end\n",
    "            W[i] = @trace(mvnormal(μ,Σ), (:W,i))\n",
    "            obs[(:W,i)] = W[i]\n",
    "            \n",
    "            #Hidden Biases\n",
    "            μ2 = ones(k[i])\n",
    "            Σ2 = Diagonal([σ₂ for i=1:length(μ2)])\n",
    "            b[i] = @trace(mvnormal(μ2,Σ2), (:b,i))\n",
    "            obs[(:b,i)] = b[i]\n",
    "        else\n",
    "            #Output Weights\n",
    "            μₒ = zeros(k[l])\n",
    "            Σₒ = Diagonal([σ₃ for i=1:length(μₒ)])\n",
    "            W[i] = @trace(mvnormal(μₒ,Σₒ), (:W,i))\n",
    "            obs[(:W,i)] = W[i]\n",
    "\n",
    "            #Output Bias\n",
    "            μ2ₒ = ones(1)\n",
    "            Σ2ₒ = Diagonal([1.0 for i=1:length(μ2ₒ)])\n",
    "            b[i] = @trace(mvnormal(μ2ₒ,Σ2ₒ), (:b,i))\n",
    "            obs[(:b,i)] = b[i]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #Return Network Scores for X\n",
    "    scores = G(x,obs)\n",
    "    scores = Flux.σ.(scores)\n",
    "    \n",
    "    #Regression Likelihood\n",
    "    y = @trace(mvnormal(vec(scores), Diagonal([σᵧ for i=1:length(x[1,:])])), (:y))\n",
    "\n",
    "    return scores\n",
    "    \n",
    "end;\n",
    "\n",
    "#(best_trace,) = generate(interpolator, (x,), obs)\n",
    "#println(best_trace[:τ₁])\n",
    "#println(best_trace[:τ₂])\n",
    "#println(best_trace[:τ₃])\n",
    "\n",
    "test_scores = interpolator(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register Observed Data - Bernoulli\n",
    "obs_master = choicemap()::ChoiceMap\n",
    "obs_master[:y] = y\n",
    "obs = obs_master;\n",
    "\n",
    "scores = []\n",
    "mses = []\n",
    "ks = []\n",
    "best_traces = []\n",
    "(best_trace,) = generate(interpolator, (x,), obs)\n",
    "best_score = get_score(best_trace)\n",
    "best_pred_y = Flux.σ.(G(x, best_trace))\n",
    "best_mse = mse_regression(best_pred_y, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------\n",
    "#Test Likelihood\n",
    "#----------------\n",
    "function likelihood(best_trace, best_mse, best_score)\n",
    "    obs = obs_master;\n",
    "    (trace,) = generate(interpolator, (x,), obs)\n",
    "    \n",
    "    pred_y = Flux.σ.(G(x, trace))\n",
    "    mse = mse_regression(pred_y, y)\n",
    "    score = get_score(trace)\n",
    "    \n",
    "    if mse > best_mse\n",
    "        best_mse = mse\n",
    "        best_score = score\n",
    "        best_trace = trace\n",
    "        best_pred_y = pred_y\n",
    "    end\n",
    "    push!(scores,score)\n",
    "    push!(mses,mse)\n",
    "    return(best_trace, best_mse, best_score)\n",
    "end;\n",
    "\n",
    "for i=1:10000\n",
    "    best_trace, best_mse, best_score = likelihood(best_trace, best_mse, best_score)\n",
    "end\n",
    "\n",
    "PyPlot.scatter(mses, scores)\n",
    "plt.title(\"Comparing Classifier Accuracy to Log Likelihood\")\n",
    "plt.xlabel(\"Classifier MSE\")\n",
    "plt.ylabel(\"Log Likelihood\")\n",
    "#plt.ylim(-100,1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------\n",
    "#RJMCMC - using NUTS\n",
    "#--------------------\n",
    "traces = []\n",
    "scores = []\n",
    "acc = []\n",
    "acc_l = []\n",
    "acc_w = []\n",
    "l_results = []\n",
    "epss = []\n",
    "\n",
    "function within_move(trace, iters, obs, prev_trace)\n",
    "    selection = select_selection(trace)\n",
    "    (new_trace, hmc_score) = NUTS(trace, selection, false, obs, iters, iters, prev_trace)\n",
    "    #println(hmc_score)\n",
    "    if rand(Uniform(0,1)) < exp(hmc_score)\n",
    "        trace = new_trace\n",
    "        accepted = 1.0\n",
    "        println(\"Accepted\")\n",
    "    else\n",
    "        trace = prev_trace\n",
    "        accepted = 0.0\n",
    "        #println(\"Not Accepted\")\n",
    "    end\n",
    "    #push!(traces, trace)\n",
    "    push!(acc, accepted)\n",
    "    push!(acc_w, accepted)\n",
    "    #accepted && println(\"Within accepted\")\n",
    "    return trace\n",
    "end\n",
    "\n",
    "function layer_move(trace, iters, obs, prev_trace)\n",
    "    \n",
    "    #Determine birth or death\n",
    "    current_l = trace[:l]\n",
    "    \n",
    "    if current_l == last(l_list)\n",
    "        move_type = 0\n",
    "    elseif current_l == l_list[1]\n",
    "        move_type = 1\n",
    "    else\n",
    "        move_type = bernoulli(0.5)\n",
    "    end\n",
    "    move = \"Empty\"\n",
    "    \n",
    "    obs_master = choicemap()::ChoiceMap\n",
    "    obs_master[:y] = y\n",
    "\n",
    "    #HMC Move 1\n",
    "    selection = select_selection(trace)\n",
    "    hmc1_trace = trace\n",
    "    (hmc1_trace, hmc1_score) = NUTS(hmc1_trace, selection, false, obs, iters, iters, prev_trace)\n",
    "\n",
    "    #RJ Move\n",
    "    if move_type == 1\n",
    "        move = \"Birth\"\n",
    "        rj_trace = layer_birth(hmc1_trace)\n",
    "    else\n",
    "        move = \"Death\"\n",
    "        rj_trace = layer_death(hmc1_trace)\n",
    "    end\n",
    "\n",
    "    #HMC Move 2\n",
    "    hmc2_trace = rj_trace\n",
    "    (hmc2_trace, hmc2_score) = NUTS(hmc2_trace, selection, false, obs, iters, iters, hmc2_trace)\n",
    "\n",
    "    score1 = get_score(prev_trace)\n",
    "    score2 = get_score(hmc2_trace)\n",
    "    logscore = (score2 - score1)\n",
    "    score = exp(logscore) #+ hmc1_score - hmc2_score)\n",
    "    #println(\"$move: $score\")\n",
    "    \n",
    "    if rand(Uniform(0,1)) < score\n",
    "        accepted = true\n",
    "        trace = hmc2_trace\n",
    "        println(\"New ks accepted! Current ks: $trace[:l]\")\n",
    "    else\n",
    "        println(\"Sticking with the old l!\")\n",
    "        accepted = false\n",
    "        trace = prev_trace\n",
    "    end\n",
    "\n",
    "    #println(\"$move Old Trace: $score1; Pre-HMC: $score_test; Post-HMC: $score2\")\n",
    "        \n",
    "    push!(acc, accepted)\n",
    "    push!(acc_l, accepted)\n",
    "    push!(scores, score)\n",
    "    return trace\n",
    "end\n",
    "\n",
    "\n",
    "function rjmcmc(starting_trace, iters)\n",
    "    trace = starting_trace\n",
    "    l = trace[:l]\n",
    "    ks = [trace[(:k,i)] for i=1:trace[:l]]\n",
    "    println(\"Beginning RJMCMC\")\n",
    "    println(\"Starting ks: $ks\")\n",
    "    println(\"--------------------------------\")\n",
    "\n",
    "    for i=1:iters\n",
    "        l = trace[:l]\n",
    "        obs = obs_master;\n",
    "        if i%10 == 0\n",
    "            #println(\"Epoch $i Acceptance Prob: $(sum(acc)/length(acc))\")\n",
    "            #println(\"Epoch $i layer count: $l, ks: $ks\")\n",
    "            println(\"Epoch $i Within Acceptance Prob: $(sum(acc_w)/length(acc_w))\")\n",
    "            #println(\"Epoch $i Layer Acceptance Prob: $(sum(acc_l)/length(acc_l))\")\n",
    "            #println([trace[(:k,i)] for i=1:trace[:l]])\n",
    "        end\n",
    "        \n",
    "        #Gibbs sampling for hyperparameters\n",
    "        prev_trace = trace\n",
    "        trace, obs = select_hyperparameters(prev_trace, obs)\n",
    "        \n",
    "        #Indicator variable for move type\n",
    "        u = rand(Uniform(0,1))\n",
    "        if u > 1.0\n",
    "            (trace) = layer_move(trace, 10, obs, prev_trace)\n",
    "        else\n",
    "            (trace) = within_move(trace, 100, obs, prev_trace)\n",
    "        end\n",
    "        push!(traces, trace)\n",
    "        push!(scores, get_score(trace))\n",
    "        push!(l_results, trace[:l])\n",
    "    end\n",
    "    println(\"Finished\")\n",
    "end\n",
    " \n",
    "runs = 2\n",
    "\n",
    "obs_master = choicemap()::ChoiceMap\n",
    "obs_master[:y] = y\n",
    "obs = obs_master;\n",
    "(starting_trace,) = generate(interpolator, (x,), obs)\n",
    "#starting_trace = best_trace\n",
    "\n",
    "include(\"NUTS.jl\");\n",
    "rjmcmc(starting_trace,runs);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
