{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification of MNIST dataset \n",
    "using Gen\n",
    "using PyPlot\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "using Distances\n",
    "using Flux\n",
    "using StatsBase\n",
    "include(\"hmc_mod.jl\")\n",
    "include(\"helper_functions.jl\")\n",
    "include(\"rj_proposals.jl\")\n",
    "include(\"mnist.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------\n",
    "#Hyperparameters and Helper Functions\n",
    "#------------------------------------\n",
    "\n",
    "#Select Network Goal\n",
    "network = \"classifier\"\n",
    "#network = \"interpolator\"\n",
    "\n",
    "#Data hyperparameters\n",
    "n = 10 #Number of samples\n",
    "c = 10 #Number of classes\n",
    "d = 784 #Input dimension\n",
    "N = n*10 #Total samples\n",
    "\n",
    "#Network hyperparameters\n",
    "α = 6 #Gamma Scale for Hyperparameters\n",
    "\n",
    "#Node hyperparameters\n",
    "k_range = 12 #Maximum number of neurons per layer\n",
    "k_list = [Int(i) for i in 1:k_range]\n",
    "k_real = 2\n",
    "\n",
    "#Layer hyperparameters\n",
    "l_range = 5 #Maximum number of layers in the network\n",
    "l_list = [Int(i) for i in 1:l_range]\n",
    "l_real = 1\n",
    "\n",
    "#NUTS\n",
    "Δmax = 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the MNIST Data\n",
    "x_total, y_total = load_mnist_test_set()\n",
    "x_raw = x_total[101:200,:]\n",
    "y_raw = y_total[101:200]\n",
    "a = countmap(y_raw)\n",
    "x = transpose(x_raw)\n",
    "size(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run PCA to lower dimensionality of MNIST (later, for now try big params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×100 Array{Float64,2}:\n",
       " 0.0           0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0           1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 6.275e-321    0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0           0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0           0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0           0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0           0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 2.86118e-269  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0           0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0           0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bayesian Neural Net\n",
    "function G(x, trace)\n",
    "    activation = relu\n",
    "    layers = trace[:l]\n",
    "    ks = [trace[(:k,i)] for i=1:layers]\n",
    "    for i=1:layers\n",
    "        in_dim, out_dim = layer_unpacker(i, layers, ks)\n",
    "        W = reshape(trace[(:W,i)], out_dim, in_dim)\n",
    "        b = reshape(trace[(:b,i)], trace[(:k,i)])\n",
    "        nn = Dense(W, b, activation)\n",
    "        x = nn(x)\n",
    "    end\n",
    "    \n",
    "    Wₒ = reshape(trace[(:W,layers+1)], 10, ks[layers])\n",
    "    bₒ = reshape(trace[(:b,layers+1)], 10)\n",
    "    \n",
    "    nn_out = Dense(Wₒ, bₒ)\n",
    "    return nn_out(x)\n",
    "end;\n",
    "\n",
    "@gen function classifier(x::Array{Float64})\n",
    "    \n",
    "    #Create a blank choicemap\n",
    "    obs = choicemap()::ChoiceMap\n",
    "    \n",
    "    #Draw number of layers\n",
    "    #l ~ categorical([1/length(l_list) for i=1:length(l_list)])\n",
    "    l ~ categorical([0.0,0.0,0.0,0.0,1.0])\n",
    "    l_real = l\n",
    "    obs[:l] = l\n",
    "    \n",
    "    #Create individual weight and bias vectors\n",
    "    #Loop through hidden layers\n",
    "    k = [Int(0) for i=1:l+1]\n",
    "    for i=1:l\n",
    "        #k[i] = @trace(categorical([1/length(k_list) for i=1:length(k_list)]), (:k,i))\n",
    "        k[i] = @trace(categorical([0.0,0.0,0.0,0.0,1.0]), (:k,i))\n",
    "        obs[(:k,i)] = k[i]\n",
    "    end\n",
    "    k[l+1] = @trace(categorical([0.0,0.0,0.0,0.0,1.0]), (:k,l+1))\n",
    "    obs[(:k,l+1)] = 5 #k[l+1]\n",
    "    \n",
    "    α = 0.001 #Gamma Scale for Hyperparameters\n",
    "    \n",
    "    ω₁ = 100\n",
    "    ω₂ = (sum([obs[(:k,i)] for i=1:l]))*100 #Neal (1996): Scaling relationship to # of hidden units\n",
    "    τ₁ ~ gamma(ω₁,α) #Hidden Weights\n",
    "    τ₂ ~ gamma(ω₁,α) #Hidden Biases\n",
    "    τ₃ ~ gamma(ω₂,α) #Output Weights\n",
    "    #τᵧ ~ gamma(ωᵧ,α) #Noise Parameter for y\n",
    "    #τ₄ ~ gamma() #Output Biases - Neal uses fixed sigmas here\n",
    "    \n",
    "    #Standard Deviations\n",
    "    σ₁ = 1/τ₁\n",
    "    σ₂ = 1/τ₂\n",
    "    σ₃ = 1/τ₃\n",
    "    #σᵧ = sqrt(1/τᵧ)\n",
    "    \n",
    "    #Sample weight and parameter vectors\n",
    "    W = [zeros(k[i]) for i=1:l+1]\n",
    "    b = [zeros(k[i]) for i=1:l+1]\n",
    "    for i=1:l+1\n",
    "        if i == 1\n",
    "            h = Int(d * k[i])\n",
    "        else\n",
    "            h = Int(k[i-1] * k[i])\n",
    "        end\n",
    "\n",
    "        if i<=l\n",
    "            #Hidden Weights\n",
    "            μ = zeros(h)\n",
    "            Σ = Diagonal([σ₁ for i=1:length(μ)])\n",
    "            W[i] = @trace(mvnormal(μ,Σ), (:W,i))\n",
    "            obs[(:W,i)] = W[i]\n",
    "            \n",
    "            #Hidden Biases\n",
    "            μ2 = zeros(k[i])\n",
    "            Σ2 = Diagonal([σ₂ for i=1:length(μ2)])\n",
    "            b[i] = @trace(mvnormal(μ2,Σ2), (:b,i))\n",
    "            obs[(:b,i)] = b[i]\n",
    "        else\n",
    "            #Output Weights\n",
    "            μₒ = zeros(k[l]*c)\n",
    "            Σₒ = Diagonal([σ₃ for i=1:length(μₒ)])\n",
    "            W[i] = @trace(mvnormal(μₒ,Σₒ), (:W,i))\n",
    "            obs[(:W,i)] = W[i]\n",
    "\n",
    "            #Output Bias\n",
    "            μ2ₒ = zeros(c)\n",
    "            Σ2ₒ = Diagonal([1.0 for i=1:length(μ2ₒ)])\n",
    "            b[i] = @trace(mvnormal(μ2ₒ,Σ2ₒ), (:b,i))\n",
    "            obs[(:b,i)] = b[i]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #Return Network Scores for X\n",
    "    scores = G(x,obs)\n",
    "    scores = softmax(scores,dims=1)\n",
    "    \n",
    "    #Logistic Regression Likelihood\n",
    "    y = zeros(length(scores))\n",
    "    for j=1:N\n",
    "        score_vec = scores[:,j]\n",
    "        y[j] = @trace(categorical(score_vec), (:y,j))\n",
    "    end\n",
    "\n",
    "    return scores\n",
    "    \n",
    "end;\n",
    "\n",
    "#(best_trace,) = generate(classifier, (x,), obs)\n",
    "#println(best_trace[:τ₁])\n",
    "#println(best_trace[:τ₂])\n",
    "#println(best_trace[:τ₃])\n",
    "\n",
    "test = test_scores = classifier(x)\n",
    "#test_labels = data_labeller(test_scores)\n",
    "#test_acc = sum([classes[i] == test_labels[i] for i=1:length(classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×100 Array{Float64,2}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0           0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  1.01971e-206  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0           0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0           0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0           0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0           0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0           0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0           0.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0           1.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  1.73384e-175  0.0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:,1]\n",
    "softmax(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
