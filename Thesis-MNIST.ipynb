{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling TensorBoardLogger [899adc3e-224a-11e9-021f-63837185c80f]\n",
      "└ @ Base loading.jl:1260\n",
      "┌ Info: Precompiling MLDatasets [eb30cadb-4394-5ae3-aed4-317e484a6458]\n",
      "└ @ Base loading.jl:1260\n",
      "┌ Info: Precompiling DrWatson [634d3b9d-ee7a-5ddf-bec9-22491ea816e1]\n",
      "└ @ Base loading.jl:1260\n"
     ]
    }
   ],
   "source": [
    "## Classification of MNIST dataset \n",
    "## with the convolutional neural network know as LeNet5.\n",
    "## This script also combines various\n",
    "## packages from the Julia ecosystem  with Flux.\n",
    "using Flux\n",
    "using Flux.Data: DataLoader\n",
    "using Flux.Optimise: Optimiser, WeightDecay\n",
    "using Flux: onehotbatch, onecold, logitcrossentropy\n",
    "using Statistics, Random\n",
    "using Parameters: @with_kw\n",
    "using Logging: with_logger, global_logger\n",
    "using TensorBoardLogger: TBLogger, tb_overwrite, set_step!, set_step_increment!\n",
    "import ProgressMeter\n",
    "import MLDatasets\n",
    "import DrWatson: savename, struct2dict\n",
    "import BSON\n",
    "using CUDAapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet5 \"constructor\". \n",
    "# The model can be adapted to any image size\n",
    "# and number of output classes.\n",
    "function LeNet5(; imgsize=(28,28,1), nclasses=10) \n",
    "    out_conv_size = (imgsize[1]÷4 - 3, imgsize[2]÷4 - 3, 16)\n",
    "    \n",
    "    return Chain(\n",
    "            x -> reshape(x, imgsize..., :),\n",
    "            Conv((5, 5), imgsize[end]=>6, relu),\n",
    "            MaxPool((2, 2)),\n",
    "            Conv((5, 5), 6=>16, relu),\n",
    "            MaxPool((2, 2)),\n",
    "            x -> reshape(x, :, size(x, 4)),\n",
    "            Dense(prod(out_conv_size), 120, relu), \n",
    "            Dense(120, 84, relu), \n",
    "            Dense(84, nclasses)\n",
    "          )\n",
    "end\n",
    "\n",
    "#The final two layers have inference run over them.\n",
    "#Build a two-layer network to attach to the end of the frozen LeNet5\n",
    "function finallayer(x, W₁, b₁, W₂, b₂, k::Int)\n",
    "    W₁ = reshape(W₁, k, 120)\n",
    "    W₂ = reshape(W₂, 10, k)\n",
    "    b₁ = reshape(b₁, k)\n",
    "    b₂ = reshape(b₂, 10)\n",
    "    \n",
    "    nn = Chain(Dense(W₁, b₁, relu),\n",
    "               Dense(W₂, b₂))\n",
    "end\n",
    "\n",
    "#This is the flexible final Fully-Connected layer to run inference over\n",
    "@gen function classifier(x::Array{Float64})\n",
    "    α = 0.09 #\"Regularization\" Term\n",
    "    σₖ = sqrt(1/α) #Gaussian Variance\n",
    "    \n",
    "    k ~ categorical([1/length(k_list) for i=1:length(k_list)])\n",
    "    k_real = k_list[k]\n",
    "    \n",
    "    #Create individual weight and bias vectors\n",
    "    m = mₖ(k_real)\n",
    "    h = 120*k_real\n",
    "    \n",
    "    #Hidden Weights\n",
    "    μ₁ = zeros(h)\n",
    "    Σ₁ = Diagonal([σₖ for i=1:length(μ₁)])\n",
    "    Wₕ ~ mvnormal(μ₁,Σ₁)\n",
    "    \n",
    "    #Hidden Bias\n",
    "    μ₂ = ones(k)\n",
    "    Σ₂ = Diagonal([σₖ for i=1:length(μ₂)])\n",
    "    bₕ ~ mvnormal(μ₂,Σ₂)\n",
    "    \n",
    "    #Output Weights\n",
    "    μ₃ = zeros(k)\n",
    "    Σ₃ = Diagonal([σₖ for i=1:length(μ₃)])\n",
    "    Wₒ ~ mvnormal(μ₃,Σ₃)\n",
    "    \n",
    "    #Output Bias\n",
    "    μ₄ = ones(10)\n",
    "    Σ₄ = Diagonal([σₖ for i=1:length(μ₄)])\n",
    "    bₒ ~ mvnormal(μ₄,Σ₄)\n",
    "    \n",
    "    #Return Network Scores for X\n",
    "    scores = G(x,Wₕ,bₕ,Wₒ,bₒ,k_real)\n",
    "    scores = Flux.σ.(scores)\n",
    "    \n",
    "    #Logistic Regression Likelihood\n",
    "    y = @trace(mvnormal(vec(scores), Diagonal([0.1 for i=1:length(x[1,:])])), (:y))\n",
    "    \n",
    "    return y\n",
    "end;\n",
    "\n",
    "classifier(x);\n",
    "\n",
    "function get_data(args)\n",
    "    xtrain, ytrain = MLDatasets.MNIST.traindata(Float32, dir=args.datapath)\n",
    "    xtest, ytest = MLDatasets.MNIST.testdata(Float32, dir=args.datapath)\n",
    "\n",
    "    xtrain = reshape(xtrain, 28, 28, 1, :)\n",
    "    xtest = reshape(xtest, 28, 28, 1, :)\n",
    "\n",
    "    ytrain, ytest = onehotbatch(ytrain, 0:9), onehotbatch(ytest, 0:9)\n",
    "\n",
    "    train_loader = DataLoader(xtrain, ytrain, batchsize=args.batchsize, shuffle=true)\n",
    "    test_loader = DataLoader(xtest, ytest,  batchsize=args.batchsize)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "end\n",
    "\n",
    "loss(ŷ, y) = logitcrossentropy(ŷ, y)\n",
    "\n",
    "function eval_loss_accuracy(loader, model, device)\n",
    "    l = 0f0\n",
    "    acc = 0\n",
    "    ntot = 0\n",
    "    for (x, y) in loader\n",
    "        x, y = x |> device, y |> device\n",
    "        ŷ = model(x)\n",
    "        l += loss(ŷ, y) * size(x)[end]        \n",
    "        acc += sum(onecold(ŷ |> cpu) .== onecold(y |> cpu))\n",
    "        ntot += size(x)[end]\n",
    "    end\n",
    "    return (loss = l/ntot |> round4, acc = acc/ntot*100 |> round4)\n",
    "end\n",
    "\n",
    "## utility functions\n",
    "\n",
    "num_params(model) = sum(length, Flux.params(model)) \n",
    "\n",
    "round4(x) = round(x, digits=4)\n",
    "\n",
    "\n",
    "# arguments for the `train` function \n",
    "@with_kw mutable struct Args\n",
    "    η = 3e-4             # learning rate\n",
    "    λ = 0                # L2 regularizer param, implemented as weight decay\n",
    "    batchsize = 128      # batch size\n",
    "    epochs = 20          # number of epochs\n",
    "    seed = 0             # set seed > 0 for reproducibility\n",
    "    cuda = true          # if true use cuda (if available)\n",
    "    infotime = 1 \t     # report every `infotime` epochs\n",
    "    checktime = 5        # Save the model every `checktime` epochs. Set to 0 for no checkpoints.\n",
    "    tblogger = false      # log training with tensorboard\n",
    "    savepath = nothing    # results path. If nothing, construct a default path from Args. If existing, may overwrite\n",
    "    datapath = joinpath(homedir(), \"Datasets\", \"MNIST\") # data path: change to your data directory \n",
    "end\n",
    "\n",
    "function train(; kws...)\n",
    "    args = Args(; kws...)\n",
    "    args.seed > 0 && Random.seed!(args.seed)\n",
    "    use_cuda = args.cuda && CUDAapi.has_cuda_gpu()\n",
    "    if use_cuda\n",
    "        device = gpu\n",
    "        @info \"Training on GPU\"\n",
    "    else\n",
    "        device = cpu\n",
    "        @info \"Training on CPU\"\n",
    "    end\n",
    "\n",
    "    ## DATA\n",
    "    train_loader, test_loader = get_data(args)\n",
    "    @info \"Dataset MNIST: $(train_loader.nobs) train and $(test_loader.nobs) test examples\"\n",
    "\n",
    "    ## MODEL AND OPTIMIZER\n",
    "    model = LeNet5() |> device\n",
    "    @info \"LeNet5 model: $(num_params(model)) trainable params\"    \n",
    "    \n",
    "    ps = Flux.params(model)  \n",
    "\n",
    "    opt = ADAM(args.η) \n",
    "    if args.λ > 0 \n",
    "        opt = Optimiser(opt, WeightDecay(args.λ))\n",
    "    end\n",
    "    \n",
    "    ## LOGGING UTILITIES\n",
    "    if args.savepath == nothing\n",
    "        experiment_folder = savename(\"lenet\", args, scientific=4,\n",
    "                    accesses=[:batchsize, :η, :seed, :λ]) # construct path from these fields\n",
    "        args.savepath = joinpath(\"runs\", experiment_folder)\n",
    "    end\n",
    "    if args.tblogger \n",
    "        tblogger = TBLogger(args.savepath, tb_overwrite)\n",
    "        set_step_increment!(tblogger, 0) # 0 auto increment since we manually set_step!\n",
    "        @info \"TensorBoard logging at \\\"$(args.savepath)\\\"\"\n",
    "    end\n",
    "    \n",
    "    function report(epoch)\n",
    "        train = eval_loss_accuracy(train_loader, model, device)\n",
    "        test = eval_loss_accuracy(test_loader, model, device)        \n",
    "        println(\"Epoch: $epoch   Train: $(train)   Test: $(test)\")\n",
    "        if args.tblogger\n",
    "            set_step!(tblogger, epoch)\n",
    "            with_logger(tblogger) do\n",
    "                @info \"train\" loss=train.loss  acc=train.acc\n",
    "                @info \"test\"  loss=test.loss   acc=test.acc\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    ## TRAINING\n",
    "    @info \"Start Training\"\n",
    "    report(0)\n",
    "    for epoch in 1:args.epochs\n",
    "        p = ProgressMeter.Progress(length(train_loader))\n",
    "\n",
    "        for (x, y) in train_loader\n",
    "            x, y = x |> device, y |> device\n",
    "            gs = Flux.gradient(ps) do\n",
    "                ŷ = model(x)\n",
    "                loss(ŷ, y)\n",
    "            end\n",
    "            Flux.Optimise.update!(opt, ps, gs)\n",
    "            ProgressMeter.next!(p)   # comment out for no progress bar\n",
    "        end\n",
    "        \n",
    "        epoch % args.infotime == 0 && report(epoch)\n",
    "        if args.checktime > 0 && epoch % args.checktime == 0\n",
    "            !ispath(args.savepath) && mkpath(args.savepath)\n",
    "            modelpath = joinpath(args.savepath, \"model.bson\") \n",
    "            let model=cpu(model), args=struct2dict(args)\n",
    "                BSON.@save modelpath model epoch args\n",
    "            end\n",
    "            @info \"Model saved in \\\"$(modelpath)\\\"\"\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
