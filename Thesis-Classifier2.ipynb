{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "using Gen\n",
    "using PyPlot\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Flux\n",
    "using Random\n",
    "using Distances\n",
    "include(\"hmc_mod.jl\")\n",
    "include(\"helper_functions.jl\")\n",
    "include(\"rj_proposals.jl\")\n",
    "include(\"NUTS.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------\n",
    "#Hyperparameters and Helper Functions\n",
    "#------------------------------------\n",
    "\n",
    "#Select Network Goal\n",
    "network = \"classifier\"\n",
    "#network = \"interpolator\"\n",
    "\n",
    "#Data hyperparameters\n",
    "n = 20 #Number of samples per mode (classifier)\n",
    "m = 4 #Number of modes (classifier)\n",
    "d = 2 #Input dimension\n",
    "N = n*m #Total samples\n",
    "σₐ = 0.02 #Mode variance (classifier)\n",
    "\n",
    "#Network hyperparameters\n",
    "α = 6 #Gamma Scale for Hyperparameters\n",
    "\n",
    "#Node hyperparameters\n",
    "k_range = 12 #Maximum number of neurons per layer\n",
    "k_list = [Int(i) for i in 1:k_range]\n",
    "k_real = 2\n",
    "\n",
    "#Layer hyperparameters\n",
    "l_range = 1 #Maximum number of layers in the network\n",
    "l_list = [Int(i) for i in 1:l_range]\n",
    "l_real = 1\n",
    "\n",
    "#NUTS\n",
    "Δmax = 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "x_raw, classes = real_data_classifier(Int(N/4), 4, σₐ);\n",
    "classes = [(i+1) % 2 + 1 for i in classes]\n",
    "y_real = classes\n",
    "\n",
    "plot_data_classifier(x_raw,classes)\n",
    "x = transpose(x_raw)\n",
    "size(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Neural Net\n",
    "function G(x, trace)\n",
    "    activation = relu\n",
    "    layers = trace[:l]\n",
    "    ks = [trace[(:k,i)] for i=1:layers]\n",
    "    for i=1:layers\n",
    "        in_dim, out_dim = layer_unpacker(i, layers, ks)\n",
    "        W = reshape(trace[(:W,i)], out_dim, in_dim)\n",
    "        b = reshape(trace[(:b,i)], trace[(:k,i)])\n",
    "        nn = Dense(W, b, activation)\n",
    "        x = nn(x)\n",
    "    end\n",
    "    \n",
    "    Wₒ = reshape(trace[(:W,layers+1)], 1, ks[layers])\n",
    "    bₒ = reshape(trace[(:b,layers+1)], 1)\n",
    "    \n",
    "    nn_out = Dense(Wₒ, bₒ)\n",
    "    return nn_out(x)\n",
    "end;\n",
    "\n",
    "@gen function classifier(x::Array{Float64})\n",
    "    \n",
    "    #Create a blank choicemap\n",
    "    obs = choicemap()::ChoiceMap\n",
    "    \n",
    "    #Draw number of layers\n",
    "    l ~ categorical([1/length(l_list) for i=1:length(l_list)])\n",
    "    l_real = l\n",
    "    obs[:l] = l\n",
    "    \n",
    "    #Create individual weight and bias vectors\n",
    "    #Loop through hidden layers\n",
    "    k = [Int(0) for i=1:l+1]\n",
    "    for i=1:l\n",
    "        k[i] = @trace(categorical([1/length(k_list) for i=1:length(k_list)]), (:k,i))\n",
    "        obs[(:k,i)] = k[i]\n",
    "    end\n",
    "    k[l+1] = @trace(categorical([1.0]), (:k,l+1))\n",
    "    obs[(:k,l+1)] = k[l+1]\n",
    "    \n",
    "    α = 0.001 #Gamma Scale for Hyperparameters\n",
    "    \n",
    "    ω₁ = 100\n",
    "    ω₂ = (sum([obs[(:k,i)] for i=1:l]))*100 #Neal (1996): Scaling relationship to # of hidden units\n",
    "    τ₁ ~ gamma(ω₁,α) #Hidden Weights\n",
    "    τ₂ ~ gamma(ω₁,α) #Hidden Biases\n",
    "    τ₃ ~ gamma(ω₂,α) #Output Weights\n",
    "    #τᵧ ~ gamma(ωᵧ,α) #Noise Parameter for y\n",
    "    #τ₄ ~ gamma() #Output Biases - Neal uses fixed sigmas here\n",
    "    \n",
    "    #Standard Deviations\n",
    "    σ₁ = 1/τ₁\n",
    "    σ₂ = 1/τ₂\n",
    "    σ₃ = 1/τ₃\n",
    "    #σᵧ = sqrt(1/τᵧ)\n",
    "    \n",
    "    #Sample weight and parameter vectors\n",
    "    W = [zeros(k[i]) for i=1:l+1]\n",
    "    b = [zeros(k[i]) for i=1:l+1]\n",
    "    for i=1:l+1\n",
    "        if i == 1\n",
    "            h = Int(d * k[i])\n",
    "        else\n",
    "            h = Int(k[i-1] * k[i])\n",
    "        end\n",
    "\n",
    "        if i<=l\n",
    "            #Hidden Weights\n",
    "            μ = zeros(h)\n",
    "            Σ = Diagonal([σ₁ for i=1:length(μ)])\n",
    "            W[i] = @trace(mvnormal(μ,Σ), (:W,i))\n",
    "            obs[(:W,i)] = W[i]\n",
    "            \n",
    "            #Hidden Biases\n",
    "            μ2 = ones(k[i])\n",
    "            Σ2 = Diagonal([σ₂ for i=1:length(μ2)])\n",
    "            b[i] = @trace(mvnormal(μ2,Σ2), (:b,i))\n",
    "            obs[(:b,i)] = b[i]\n",
    "        else\n",
    "            #Output Weights\n",
    "            μₒ = zeros(k[l])\n",
    "            Σₒ = Diagonal([σ₃ for i=1:length(μₒ)])\n",
    "            W[i] = @trace(mvnormal(μₒ,Σₒ), (:W,i))\n",
    "            obs[(:W,i)] = W[i]\n",
    "\n",
    "            #Output Bias\n",
    "            μ2ₒ = ones(1)\n",
    "            Σ2ₒ = Diagonal([1.0 for i=1:length(μ2ₒ)])\n",
    "            b[i] = @trace(mvnormal(μ2ₒ,Σ2ₒ), (:b,i))\n",
    "            obs[(:b,i)] = b[i]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #Return Network Scores for X\n",
    "    scores = G(x,obs)\n",
    "    scores = Flux.σ.(scores)\n",
    "    \n",
    "    #Logistic Regression Likelihood\n",
    "    y = zeros(length(scores))\n",
    "    for j=1:N\n",
    "        y[j] = @trace(categorical([1-scores[j],scores[j]]), (:y,j))\n",
    "    end\n",
    "\n",
    "    return scores\n",
    "    \n",
    "end;\n",
    "\n",
    "#(best_trace,) = generate(classifier, (x,), obs)\n",
    "#println(best_trace[:τ₁])\n",
    "#println(best_trace[:τ₂])\n",
    "#println(best_trace[:τ₃])\n",
    "\n",
    "test_scores = classifier(x)\n",
    "test_labels = data_labeller(test_scores)\n",
    "test_acc = sum([classes[i] == test_labels[i] for i=1:length(classes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "#Register Observed Data - Test Likelihood\n",
    "#-----------------------------------------\n",
    "obs_master = choicemap()::ChoiceMap\n",
    "for i=1:length(classes)\n",
    "    obs_master[(:y,i)] = classes[i]\n",
    "end\n",
    "obs = obs_master;\n",
    "\n",
    "scores = []\n",
    "accs = []\n",
    "ks = []\n",
    "best_ks = []\n",
    "best_traces = []\n",
    "(best_trace,) = generate(classifier, (x,), obs)\n",
    "best_acc = 0\n",
    "best_score = get_score(best_trace)\n",
    "best_pred_y = (G(x, best_trace))\n",
    "best_pred_labels = data_labeller(best_pred_y)\n",
    "best_k = best_trace[(:k,1)]\n",
    "while best_acc <= 40\n",
    "    (best_trace,) = generate(classifier, (x,), obs)\n",
    "    best_score = get_score(best_trace)\n",
    "    best_k = best_trace[(:k,1)]\n",
    "    best_pred_y = (G(x, best_trace))\n",
    "    best_pred_labels = data_labeller(best_pred_y)\n",
    "    #best_pred_y = [best_trace[(:y,i)] for i=1:length(classes)]\n",
    "    best_acc = sum([classes[i] == best_pred_labels[i] for i=1:length(classes)])\n",
    "end;\n",
    "#println(best_trace[:τ₁])\n",
    "#println(best_trace[:τ₂])\n",
    "#println(best_trace[:τ₃])\n",
    "#println(best_trace[:τᵧ])\n",
    "\n",
    "println(best_acc)\n",
    "println(get_score(best_trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------\n",
    "#Test Likelihood\n",
    "#----------------\n",
    "scores = []\n",
    "accs = []\n",
    "ks = []\n",
    "best_ks = []\n",
    "best_traces = []\n",
    "function likelihood(best_trace, best_acc, best_score, best_k)\n",
    "    obs = obs_master;\n",
    "    #obs[(:k,1)] = 2\n",
    "    (trace,) = generate(classifier, (x,), obs)\n",
    "    \n",
    "    pred_y = (G(x, trace))\n",
    "    pred_labels = data_labeller(pred_y)\n",
    "    acc = sum([classes[i] == pred_labels[i] for i=1:length(classes)])\n",
    "    \n",
    "    #pred_y = [trace[(:y,i)] for i=1:length(classes)]\n",
    "    #acc = sum([classes[i] == pred_y[i] for i=1:length(classes)])\n",
    "    score = get_score(trace)\n",
    "    #(score, retval) = assess(lh, (x,trace), obs_master)\n",
    "    \n",
    "    #println(score)\n",
    "    if acc > best_acc\n",
    "        best_acc = acc\n",
    "        best_score = score\n",
    "        best_trace = trace\n",
    "        best_pred_y = pred_y\n",
    "        best_k = best_trace[(:k,1)]\n",
    "    end\n",
    "    #push!(best_ks,best_k)\n",
    "    push!(scores,score)\n",
    "    push!(accs,acc)\n",
    "    #push!(ls, l)\n",
    "    #println(best_score)\n",
    "    return(best_trace, best_acc, best_score, best_k)\n",
    "end;\n",
    "\n",
    "for i=1:10000\n",
    "    best_trace, best_acc, best_score, best_k = likelihood(best_trace, best_acc, best_score, best_k)\n",
    "    push!(best_ks, best_k)\n",
    "end\n",
    "\n",
    "PyPlot.scatter(accs, scores)\n",
    "plt.title(\"Comparing Classifier Accuracy to Log Likelihood\")\n",
    "plt.xlabel(\"Classifier Accuracy\")\n",
    "plt.ylabel(\"Log Likelihood\")\n",
    "plt.ylim(-250,1)\n",
    "plt.legend()\n",
    "#print(best_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_master = choicemap()::ChoiceMap\n",
    "for i=1:length(classes)\n",
    "    obs_master[(:y,i)] = classes[i]\n",
    "end\n",
    "obs = obs_master;\n",
    "(trace,) = generate(classifier, (x,), obs);\n",
    "args = get_args(trace)\n",
    "retval_grad = accepts_output_grad(get_gen_fn(trace)) ? zero(get_retval(trace)) : nothing\n",
    "argdiffs = map((_) -> NoChange(), args)\n",
    "selection = select_selection_NUTS(trace)\n",
    "gen_fn = trace.gen_fn\n",
    "state = GFBackpropTraceState(trace, selection, gen_fn.params, tape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------\n",
    "#RJMCMC - using NUTS\n",
    "#--------------------\n",
    "traces = []\n",
    "scores = []\n",
    "scores2 = []\n",
    "acc = []\n",
    "acc_a = []\n",
    "acc_w = []\n",
    "acc_l = []\n",
    "k_results = []\n",
    "epss = []\n",
    "aL = 20\n",
    "wL = 1\n",
    "\n",
    "function within_move(trace, iters, obs, prev_trace)\n",
    "    for i=1:iters\n",
    "        eps = uniform(0.01,0.04)\n",
    "        selection = select_selection(trace)\n",
    "        (new_trace, hmc_score) = NUTS(trace, selection, eps, false, obs, 1, prev_trace)\n",
    "        #println(hmc_score)\n",
    "        if rand(Uniform(0,1)) < exp(hmc_score)\n",
    "            trace = new_trace\n",
    "            accepted = true\n",
    "            #println(\"Accepted\")\n",
    "        else\n",
    "            trace = prev_trace\n",
    "            accepted = false\n",
    "            #println(\"Not Accepted\")\n",
    "        end\n",
    "        push!(traces, trace)\n",
    "        push!(acc, accepted)\n",
    "        push!(acc_w, accepted)\n",
    "        #accepted && println(\"Within accepted\")\n",
    "    end\n",
    "    return trace\n",
    "end\n",
    "\n",
    "function node_move(trace, iters, obs, prev_trace)\n",
    "    \n",
    "    #Determine birth or death\n",
    "    current_l = trace[:l]\n",
    "    layer = rand((1:current_l))\n",
    "    current_k = trace[(:k, layer)]\n",
    "    \n",
    "    if current_k == last(k_list)\n",
    "        move_type = 0\n",
    "    elseif current_k == k_list[1]\n",
    "        move_type = 1\n",
    "    else\n",
    "        move_type = bernoulli(0.5)\n",
    "    end\n",
    "    move = \"Empty\"\n",
    "    \n",
    "    obs_master = choicemap()::ChoiceMap\n",
    "    for i=1:length(classes)\n",
    "        obs_master[(:y,i)] = classes[i]\n",
    "    end\n",
    "    \n",
    "    eps = rand(Uniform(0.1,0.4))\n",
    "\n",
    "    #HMC Move 1\n",
    "    selection = select_selection(trace)\n",
    "    hmc1_trace = trace\n",
    "    (hmc1_trace, hmc1_score) = NUTS(hmc1_trace, selection, eps, false, obs, 2, prev_trace)\n",
    "\n",
    "    #RJ Move\n",
    "    if move_type == 1\n",
    "        move = \"Birth\"\n",
    "        rj_trace = node_birth(hmc1_trace, layer)\n",
    "    else\n",
    "        move = \"Death\"\n",
    "        rj_trace = node_death(hmc1_trace, layer)\n",
    "    end\n",
    "\n",
    "    #HMC Move 2\n",
    "    hmc2_trace = rj_trace\n",
    "    (hmc2_trace, hmc2_score) = NUTS(hmc2_trace, selection, eps, false, obs, 2, hmc2_trace)\n",
    "\n",
    "    #(score1,) = assess(lh, (x, trace), obs_master)\n",
    "    #(score2,) = assess(lh, (x, hmc2_trace), obs_master)\n",
    "    score1 = get_score(trace)\n",
    "    score2 = get_score(hmc2_trace)\n",
    "    logscore = (score2 - score1)\n",
    "    score = exp(logscore) * exp(hmc1_score) * exp(-hmc2_score)\n",
    "    #println(\"$move: $score\")\n",
    "    \n",
    "    if rand(Uniform(0,1)) < score\n",
    "        accepted = true\n",
    "        trace = hmc2_trace\n",
    "        new_k = [trace[(:k,i)] for i=1:trace[:l]]\n",
    "        println(\"New ks accepted! Current ks: $(new_k)\")\n",
    "    else\n",
    "        println(\"Sticking with the old k!\")\n",
    "        accepted = false\n",
    "        trace = prev_trace\n",
    "    end\n",
    "\n",
    "    #println(\"$move Old Trace: $score1; Pre-HMC: $score_test; Post-HMC: $score2\")\n",
    "        \n",
    "    push!(traces, trace)\n",
    "    push!(acc, accepted)\n",
    "    push!(acc_a, accepted)\n",
    "    push!(scores, score)\n",
    "    push!(scores2, logscore)\n",
    "    return trace\n",
    "end\n",
    "\n",
    "\n",
    "function rjmcmc(starting_trace, iters)\n",
    "    trace = starting_trace\n",
    "    l = trace[:l]\n",
    "    ks = [trace[(:k,i)] for i=1:trace[:l]]\n",
    "    println(\"Beginning RJMCMC\")\n",
    "    println(\"Starting ks: $ks\")\n",
    "    println(\"--------------------------------\")\n",
    "\n",
    "    for i=1:iters\n",
    "        l = trace[:l]\n",
    "        ks = [trace[(:k,i)] for i=1:trace[:l]]\n",
    "        #obs_master = choicemap()::ChoiceMap\n",
    "        #for i=1:length(classes)\n",
    "            #obs_master[(:y,i)] = classes[i]\n",
    "        #end\n",
    "        obs = obs_master;\n",
    "        if i%50 == 0\n",
    "            #println(\"Epoch $i Acceptance Prob: $(sum(acc)/length(acc))\")\n",
    "            #println(\"Epoch $i layer count: $l, ks: $ks\")\n",
    "            println(\"Epoch $i Within Acceptance Prob: $(sum(acc_w)/length(acc_w))\")\n",
    "            println(\"Epoch $i Across Acceptance Prob: $(sum(acc_a)/length(acc_a))\")\n",
    "            #println(\"Epoch $i Layer Acceptance Prob: $(sum(acc_l)/length(acc_l))\")\n",
    "            #println([trace[(:k,i)] for i=1:trace[:l]])\n",
    "        end\n",
    "        \n",
    "        #Gibbs sampling for hyperparameters\n",
    "        prev_trace = trace\n",
    "        trace, obs = select_hyperparameters(prev_trace, obs)\n",
    "        \n",
    "        #Indicator variable for move type\n",
    "        u = rand(Uniform(0,1))\n",
    "        if u > 0.9\n",
    "            (trace) = node_move(trace, 1, obs, prev_trace)\n",
    "        else\n",
    "            (trace) = within_move(trace, 1, obs, prev_trace)\n",
    "        end\n",
    "        push!(scores, get_score(trace))\n",
    "        push!(k_results, trace[(:k,1)])\n",
    "    end\n",
    "    println(\"Finished\")\n",
    "end\n",
    " \n",
    "runs = 100\n",
    "#starting_trace = best_trace\n",
    "\n",
    "#x_samp, y_samp = sample_data(x, y_real)\n",
    "#x_samp = transpose(x_samp)\n",
    "obs_master = choicemap()::ChoiceMap\n",
    "for i=1:length(classes)\n",
    "    obs_master[(:y,i)] = classes[i]\n",
    "end\n",
    "obs = obs_master\n",
    "obs[:l] = 1\n",
    "(starting_trace,) = generate(classifier, (x,), obs)\n",
    "#starting_trace = best_trace\n",
    "obs[:l] = starting_trace[:l]\n",
    "\n",
    "rjmcmc(starting_trace,runs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"NUTS.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(k_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(scores2, c=\"Red\")\n",
    "#plot(scores, c=\"Blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = [get_score(trace) for trace in traces]\n",
    "plot(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "for i=1:length(traces)\n",
    "    trace = traces[i]\n",
    "    pred_y = G(x,trace)\n",
    "    pred_labels = data_labeller(pred_y)\n",
    "    \n",
    "    acc = acc = sum([classes[i] == pred_labels[i] for i=1:length(classes)])\n",
    "    push!(accs,acc)\n",
    "end\n",
    "\n",
    "plot(accs)\n",
    "println(sum(accs)/length(accs))\n",
    "plt.title(\"RJMCMC Accuracy: XOR Classifier\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"# Classified Correctly (out of 200)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar Plot of k estimates\n",
    "#ks = [traces[i][:k] for i in 1:length(traces)]\n",
    "counts = [sum([k_results[i] == j for i in 1:length(k_results)]) for j in k_list]\n",
    "#actual_ks = [i for i=1:maximum(ks)]\n",
    "barlist=bar(k_list,counts)\n",
    "for i in 1:k_range\n",
    "    barlist[i].set_color([0.8,0.0,0.0])\n",
    "end\n",
    "barlist[2].set_color([0.0,0.8,0.1])\n",
    "\n",
    "plt.title(\"Counts of Values for k from RJMCMC Trace: XOR Classifier\")\n",
    "plt.xlabel(\"k value\")\n",
    "plt.ylabel(\"Occurences\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function plot_grid(data,scores,alpha=1.0)\n",
    "    PyPlot.scatter(data[:,1],data[:,2],c=scores,alpha=alpha,cmap=\"PRGn\")\n",
    "    #PyPlot.colorbar()\n",
    "end\n",
    "\n",
    "function tracegrid(traces, samples=1000, low=-2.0, high=2.0)\n",
    "    d=2\n",
    "    n=100\n",
    "    r = range(low, high, length = n)\n",
    "    \n",
    "    iter = Iterators.product((r for _ in 1:d)...)\n",
    "    grid= vec([collect(i) for i in iter])\n",
    "    grid_raw = reduce(hcat, getindex.(grid,i) for i in eachindex(grid[1]))\n",
    "    grid2 = transpose(grid_raw)\n",
    "    z_master = zeros(length(grid2[1,:]))\n",
    "    \n",
    "    for i=1:samples\n",
    "        j = rand((100,length(traces)))\n",
    "        trace = traces[j]\n",
    "        z = Flux.σ.(G(grid2,trace))[1,:]\n",
    "        z_master += (z ./ samples)\n",
    "    end\n",
    "    plot_grid(grid_raw, z_master)\n",
    "end\n",
    "\n",
    "tracegrid(traces)\n",
    "plot_data_classifier(x_raw,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "#RJMCMC - Layers and Nodes\n",
    "#OLD HMC BASED ALGORITHM\n",
    "#-------------------------\n",
    "traces = []\n",
    "scores = []\n",
    "scores2 = []\n",
    "acc = []\n",
    "acc_a = []\n",
    "acc_w = []\n",
    "acc_l = []\n",
    "k_results = []\n",
    "epss = []\n",
    "aL = 20\n",
    "wL = 1\n",
    "\n",
    "function within_move(trace, iters, obs)\n",
    "    for i=1:iters\n",
    "        eps = uniform(0.01,0.4)\n",
    "        selection = select_selection(trace)\n",
    "        (new_trace, hmc_score) = hmc_mod(trace, selection, L=wL, eps=eps, check=false, observations=obs)\n",
    "        if rand(Uniform(0,1)) < exp(hmc_score)\n",
    "            trace = new_trace\n",
    "            accepted = true\n",
    "        else\n",
    "            accepted = false\n",
    "        end\n",
    "        push!(traces, trace)\n",
    "        push!(acc, accepted)\n",
    "        push!(acc_w, accepted)\n",
    "        #accepted && println(\"Within accepted\")\n",
    "    end\n",
    "    return trace\n",
    "end\n",
    "\n",
    "function node_move(trace, iters, obs)\n",
    "    \n",
    "    #Determine birth or death\n",
    "    current_l = trace[:l]\n",
    "    layer = rand((1:current_l))\n",
    "    current_k = trace[(:k, layer)]\n",
    "    \n",
    "    if current_k == last(k_list)\n",
    "        move_type = 0\n",
    "    elseif current_k == k_list[1]\n",
    "        move_type = 1\n",
    "    else\n",
    "        move_type = bernoulli(0.5)\n",
    "    end\n",
    "    move = \"Empty\"\n",
    "    \n",
    "    obs_master = choicemap()::ChoiceMap\n",
    "    for i=1:length(classes)\n",
    "        obs_master[(:y,i)] = classes[i]\n",
    "    end\n",
    "    \n",
    "    eps = rand(Uniform(0.01,0.4))\n",
    "    \n",
    "    #Can only iter once for now\n",
    "\n",
    "    #HMC Move 1\n",
    "    selection = select_selection(trace)\n",
    "    hmc1_trace = trace\n",
    "    (hmc1_trace, hmc1_score) = hmc_mod(hmc1_trace, selection, L=aL, eps=eps, check=true, observations=obs)\n",
    "\n",
    "    #RJ Move\n",
    "    if move_type == 1\n",
    "        move = \"Birth\"\n",
    "        rj_trace = node_birth(hmc1_trace, layer)\n",
    "    else\n",
    "        move = \"Death\"\n",
    "        rj_trace = node_death(hmc1_trace, layer)\n",
    "    end\n",
    "\n",
    "    #HMC Move 2\n",
    "    hmc2_trace = rj_trace\n",
    "    (hmc2_trace, hmc2_score) = hmc_mod(hmc2_trace, selection, L=aL, eps=eps, check=true, observations=obs)\n",
    "\n",
    "    #(score1,) = assess(lh, (x, trace), obs_master)\n",
    "    #(score2,) = assess(lh, (x, hmc2_trace), obs_master)\n",
    "    score1 = get_score(trace)\n",
    "    score2 = get_score(hmc2_trace)\n",
    "    logscore = (score2 - score1)\n",
    "    score = exp(logscore) * exp(hmc1_score) * exp(-hmc2_score)\n",
    "    #println(\"$move: $score\")\n",
    "    \n",
    "    if rand(Uniform(0,1)) < score\n",
    "        accepted = true\n",
    "        trace = hmc2_trace\n",
    "        new_k = [trace[(:k,i)] for i=1:trace[:l]]\n",
    "        println(\"New ks accepted! Current ks: $(new_k)\")\n",
    "    else\n",
    "        #println(\"Sticking with the old k!\")\n",
    "        accepted = false\n",
    "    end\n",
    "\n",
    "    #println(\"$move Old Trace: $score1; Pre-HMC: $score_test; Post-HMC: $score2\")\n",
    "        \n",
    "    push!(traces, trace)\n",
    "    push!(acc, accepted)\n",
    "    push!(acc_a, accepted)\n",
    "    push!(scores, score)\n",
    "    push!(scores2, logscore)\n",
    "    return trace\n",
    "end\n",
    "\n",
    "\n",
    "function rjmcmc(starting_trace, iters)\n",
    "    trace = starting_trace\n",
    "    l = trace[:l]\n",
    "    ks = [trace[(:k,i)] for i=1:trace[:l]]\n",
    "    println(\"Beginning RJMCMC\")\n",
    "    println(\"Starting ks: $ks\")\n",
    "    println(\"--------------------------------\")\n",
    "    #@showprogress 1 \"Sampling...\" for i = 1:iters\n",
    "    for i=1:iters\n",
    "        l = trace[:l]\n",
    "        ks = [trace[(:k,i)] for i=1:trace[:l]]\n",
    "        #x_samp, y_samp = sample_data(x, y_real)\n",
    "        #x_samp = transpose(x_samp)\n",
    "        obs_master = choicemap()::ChoiceMap\n",
    "        for i=1:length(classes)\n",
    "            obs_master[(:y,i)] = classes[i]\n",
    "        end\n",
    "        obs = obs_master;\n",
    "        if i%100 == 0\n",
    "            #println(\"Epoch $i Acceptance Prob: $(sum(acc)/length(acc))\")\n",
    "            #println(\"Epoch $i layer count: $l, ks: $ks\")\n",
    "            #println(\"Epoch $i Within Acceptance Prob: $(sum(acc_w)/length(acc_w))\")\n",
    "            println(\"Epoch $i Across Acceptance Prob: $(sum(acc_a)/length(acc_a))\")\n",
    "            #println(\"Epoch $i Layer Acceptance Prob: $(sum(acc_l)/length(acc_l))\")\n",
    "            println([trace[(:k,i)] for i=1:trace[:l]])\n",
    "        end\n",
    "        \n",
    "        #Gibbs sampling for hyperparameters\n",
    "        trace, obs = select_hyperparameters(trace, obs)\n",
    "        \n",
    "        #Indicator variable for move type\n",
    "        u = rand(Uniform(0,1))\n",
    "        if u > 0.0\n",
    "            (trace) = node_move(trace, 1, obs)\n",
    "        else\n",
    "            (trace) = within_move(trace, 1, obs)\n",
    "        end\n",
    "        push!(scores, get_score(trace))\n",
    "        push!(k_results, trace[(:k,1)])\n",
    "    end\n",
    "    println(\"Finished\")\n",
    "end\n",
    " \n",
    "runs = 20000\n",
    "#starting_trace = best_trace\n",
    "\n",
    "#x_samp, y_samp = sample_data(x, y_real)\n",
    "#x_samp = transpose(x_samp)\n",
    "obs_master = choicemap()::ChoiceMap\n",
    "for i=1:length(classes)\n",
    "    obs_master[(:y,i)] = classes[i]\n",
    "end\n",
    "obs = obs_master\n",
    "obs[:l] = 1\n",
    "(starting_trace,) = generate(classifier, (x,), obs)\n",
    "#starting_trace = best_trace\n",
    "obs[:l] = starting_trace[:l]\n",
    "\n",
    "rjmcmc(starting_trace,runs);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
