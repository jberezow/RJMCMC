{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "using Gen\n",
    "using PyPlot\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Flux\n",
    "using Random\n",
    "using Distances\n",
    "using JLD\n",
    "using StatsBase\n",
    "include(\"hmc_mod.jl\")\n",
    "include(\"helper_functions.jl\")\n",
    "include(\"rj_proposals_layers.jl\")\n",
    "include(\"NUTS_CS.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready...\n"
     ]
    }
   ],
   "source": [
    "#---------------------------\n",
    "#Load Boston Housing Dataset\n",
    "#---------------------------\n",
    "data = load(\"boston.jld\")[\"boston\"]\n",
    "\n",
    "# Generating test/training sets:\n",
    "nrow, ncol = size(data)\n",
    "nrow_test  = div(nrow, 3)\n",
    "nrow_train = nrow - nrow_test\n",
    "\n",
    "x = data[:,1:13]\n",
    "y = data[:,14]\n",
    "y_raw = y\n",
    "\n",
    "dx = fit(ZScoreTransform, x, dims=1)\n",
    "StatsBase.transform!(dx, x)\n",
    "dy = fit(ZScoreTransform, y; dims=1)\n",
    "StatsBase.transform!(dy, y);\n",
    "\n",
    "x_raw = x\n",
    "x = transpose(x)\n",
    "\n",
    "#------------------------------------\n",
    "#Hyperparameters and Helper Functions\n",
    "#------------------------------------\n",
    "\n",
    "#Select Network Goal\n",
    "#network = \"classifier\"\n",
    "network = \"interpolator\"\n",
    "\n",
    "#Data hyperparameters\n",
    "n = nrow #Number of samples per mode (classifier)\n",
    "d = ncol-1 #Input dimension\n",
    "\n",
    "#Network hyperparameters\n",
    "k_real = 8 #Number of hidden nodes per layer\n",
    "k_vector = [0.0 for i=1:k_real]\n",
    "k_vector[k_real] = 1.0\n",
    "\n",
    "#Layer hyperparameters\n",
    "l_range = 4 #Maximum number of layers in the network\n",
    "l_list = [Int(i) for i in 1:l_range]\n",
    "l_real = 1;\n",
    "\n",
    "#Bayesian Neural Net\n",
    "function G(x, trace)\n",
    "    activation = sigmoid\n",
    "    layers = trace[:l]\n",
    "    ks = [trace[(:k,i)] for i=1:layers]\n",
    "    for i=1:layers\n",
    "        in_dim, out_dim = layer_unpacker(i, layers, ks)\n",
    "        W = reshape(trace[(:W,i)], out_dim, in_dim)\n",
    "        b = reshape(trace[(:b,i)], trace[(:k,i)])\n",
    "        nn = Dense(W, b, activation)\n",
    "        x = nn(x)\n",
    "    end\n",
    "    \n",
    "    Wₒ = reshape(trace[(:W,layers+1)], 1, ks[layers])\n",
    "    bₒ = reshape(trace[(:b,layers+1)], 1)\n",
    "    \n",
    "    nn_out = Dense(Wₒ, bₒ)\n",
    "    return nn_out(x)\n",
    "end;\n",
    "\n",
    "@gen function interpolator(x)\n",
    "    \n",
    "    #Create a blank choicemap\n",
    "    obs = choicemap()::ChoiceMap\n",
    "    \n",
    "    #Draw number of layers\n",
    "    l ~ categorical([1/length(l_list) for i=1:length(l_list)])\n",
    "    l_real = l\n",
    "    obs[:l] = l\n",
    "    \n",
    "    #Create individual weight and bias vectors\n",
    "    #Loop through hidden layers\n",
    "    k = [Int(0) for i=1:l+1]\n",
    "    for i=1:l\n",
    "        k[i] = @trace(categorical(k_vector), (:k,i))\n",
    "        obs[(:k,i)] = k[i]\n",
    "    end\n",
    "    k[l+1] = @trace(categorical([1.0]), (:k,l+1))\n",
    "    obs[(:k,l+1)] = k[l+1]\n",
    "    \n",
    "    ######################################\n",
    "    #New hyperparameter schedule - Jan 20#\n",
    "    ######################################\n",
    "    \n",
    "    #Standard Deviations\n",
    "    τ₁ ~ gamma(100,0.01) #(100,0.01) Hidden weights and biases\n",
    "    τ₂ ~ gamma(100*k[1],0.01) #(100*k,0.01) Output weights and biases\n",
    "    τᵧ ~ gamma(100, 0.5) #(100,0.5)\n",
    "    σ₁ = 1/τ₁\n",
    "    σ₂ = 1/τ₂\n",
    "    σᵧ = 1/τᵧ\n",
    "    \n",
    "    #Sample weight and bias vectors\n",
    "    W = [zeros(k[i]) for i=1:l+1]\n",
    "    b = [zeros(k[i]) for i=1:l+1]\n",
    "    μ = [zeros(k[i]) for i=1:l+1]\n",
    "    μb = [zeros(k[i]) for i=1:l+1]\n",
    "\n",
    "    for i=1:l+1\n",
    "        if i == 1\n",
    "            h = Int(d * k[i])\n",
    "        else\n",
    "            h = Int(k[i-1] * k[i])\n",
    "        end\n",
    "\n",
    "        if i<=l\n",
    "            #Hidden Weights\n",
    "            u = zeros(h) #Draw\n",
    "            S = Diagonal([1 for i=1:length(u)])\n",
    "            μ[i] = @trace(mvnormal(u,S), (:μ,i))\n",
    "            Σ = Diagonal([σ₁ for i=1:length(μ[i])])\n",
    "            W[i] = @trace(mvnormal(μ[i],Σ), (:W,i))\n",
    "            obs[(:W,i)] = W[i]\n",
    "            \n",
    "            #Hidden Biases\n",
    "            ub = zeros(k[i]) #Draw\n",
    "            Sb = Diagonal([1 for i=1:length(ub)])    \n",
    "            μb[i] = @trace(mvnormal(ub,Sb), (:μb,i))\n",
    "            Σ2 = Diagonal([σ₁ for i=1:length(μb[i])])\n",
    "            b[i] = @trace(mvnormal(μb[i],Σ2), (:b,i))\n",
    "            obs[(:b,i)] = b[i]\n",
    "        else\n",
    "            #Output Weights\n",
    "            u = zeros(k[l]) #Draw\n",
    "            S = Diagonal([1 for i=1:length(u)])\n",
    "            μ[i] = @trace(mvnormal(u,S), (:μ,i))\n",
    "            Σ = Diagonal([σ₂ for i=1:length(μ[i])])\n",
    "            W[i] = @trace(mvnormal(μ[i],Σ), (:W,i))\n",
    "            obs[(:W,i)] = W[i]\n",
    "\n",
    "            #Output Bias\n",
    "            ub = zeros(1) #Draw\n",
    "            Sb = Diagonal([1 for i=1:length(ub)])  \n",
    "            μb[i] = @trace(mvnormal(ub,Sb), (:μb,i))\n",
    "            Σ2 = Diagonal([σ₂ for i=1:length(μb[i])])\n",
    "            b[i] = @trace(mvnormal(μb[i],Σ2), (:b,i))\n",
    "            obs[(:b,i)] = b[i]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #Return Network Scores for X\n",
    "    scores = transpose(G(x,obs))[:,1]\n",
    "    #scores = Flux.tanh.(scores)\n",
    "    \n",
    "    #Regression Likelihood\n",
    "    y = @trace(mvnormal(vec(scores), Diagonal([σᵧ for i=1:length(x[1,:])])), (:y))\n",
    "\n",
    "    return scores\n",
    "    \n",
    "end;\n",
    "\n",
    "#Register Observed Data - Bernoulli\n",
    "obs_master = choicemap()::ChoiceMap\n",
    "obs_master[:y] = y\n",
    "obs = obs_master;\n",
    "obs[:l] = 1\n",
    "\n",
    "(base_trace,) = generate(interpolator, (x,), obs)\n",
    "println(\"Ready...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io = open(\"traces.jld\", \"r\")\n",
    "seekstart(io)\n",
    "println(read!(io, Ref(base_trace)).x[:l])\n",
    "close(io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
